{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 1, 2, 1, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((10,5), requires_grad=True)\n",
    "y = torch.randint(0,3, (10,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = nn.Linear(in_features=5, out_features=10)\n",
    "li2 = nn.Linear(10,3)\n",
    "\n",
    "li_out = li(x)\n",
    "out = li2(li_out)\n",
    "out.retain_grad()\n",
    "x.retain_grad()\n",
    "li_out.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "li2.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1918, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(out, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5113, 1.1901, 1.0210,    nan, 0.6620, 2.4949, 0.3594, 1.0085, 0.6821,\n",
       "           nan], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(out[range(10), y])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1918, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CrossEntropyLoss:\n",
    "\n",
    "    def __call__(self,\n",
    "                 y_pred: torch.Tensor,\n",
    "                 y_true: torch.Tensor\n",
    "                 ):\n",
    "        n_samples = y_pred.shape[0]\n",
    "        y_preds = y_pred - y_pred.max(1, keepdim=True).values\n",
    "        counts = y_preds.exp()\n",
    "        counts_sum = counts.sum(1, keepdim=True)\n",
    "        counts_sum_inv = counts_sum ** -1\n",
    "        probs = counts * counts_sum_inv\n",
    "        logprobs = probs.log()\n",
    "        self.out = -logprobs[range(n_samples), y_true].mean()\n",
    "        return self.out\n",
    "\n",
    "    def backward(self,\n",
    "                 y_pred: torch.Tensor,\n",
    "                 y_true: torch.Tensor\n",
    "                 ):\n",
    "        n_samples = y_pred.shape[0]\n",
    "        # softmax = F.softmax()\n",
    "        grad = F.softmax(y_pred, dim=1)\n",
    "        grad[range(n_samples), y_true] -= 1\n",
    "        grad = grad / n_samples\n",
    "        return grad\n",
    "\n",
    "    def paramerters(self):\n",
    "        return []\n",
    "    \n",
    "lo = CrossEntropyLoss()\n",
    "lo(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dlosstorch.Size([10, 3]), out torch.Size([10, 3])\n",
      "tensor([[ 0.0240,  0.0347, -0.0587],\n",
      "        [ 0.0299, -0.0671,  0.0373],\n",
      "        [-0.0646,  0.0288,  0.0357],\n",
      "        [ 0.0339, -0.0801,  0.0462],\n",
      "        [ 0.0240,  0.0305, -0.0545],\n",
      "        [ 0.0206, -0.0763,  0.0557],\n",
      "        [ 0.0295,  0.0259, -0.0554],\n",
      "        [-0.0686,  0.0274,  0.0413],\n",
      "        [-0.0670,  0.0303,  0.0366],\n",
      "        [-0.0860,  0.0463,  0.0397]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dloss = lo.backward(out, y)\n",
    "\n",
    "print(f\"shape of dloss{dloss.shape}, out {out.shape}\")\n",
    "print(dloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1641, -0.0714, -0.0778,  0.0775,  0.0259,  0.0220,  0.0946, -0.0208,\n",
       "          0.0993,  0.0915],\n",
       "        [-0.1298,  0.0790,  0.0594, -0.1384, -0.0275, -0.0181, -0.0883,  0.0483,\n",
       "         -0.1244, -0.0467],\n",
       "        [-0.0343, -0.0076,  0.0184,  0.0609,  0.0015, -0.0039, -0.0063, -0.0275,\n",
       "          0.0250, -0.0448]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li2.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1641, -0.0714, -0.0778,  0.0775,  0.0259,  0.0220,  0.0946, -0.0208,\n",
       "          0.0993,  0.0915],\n",
       "        [-0.1298,  0.0790,  0.0594, -0.1384, -0.0275, -0.0181, -0.0883,  0.0483,\n",
       "         -0.1244, -0.0467],\n",
       "        [-0.0343, -0.0076,  0.0184,  0.0609,  0.0015, -0.0039, -0.0063, -0.0275,\n",
       "          0.0250, -0.0448]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(li_out.shape, dloss.shape)\n",
    "dw = dloss.T @ li_out\n",
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(dw, li2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1244,  0.0004,  0.1240]),\n",
       " tensor([-0.1244,  0.0004,  0.1240], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dloss.sum(dim = 0)\n",
    "li2.bias.grad, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0885e-03, -4.2290e-03, -1.7008e-02, -1.9706e-02,  6.0901e-03,\n",
       "         -6.6475e-03, -1.6097e-02, -1.0117e-03, -4.1209e-03,  4.6926e-03],\n",
       "        [ 1.2462e-03,  1.3961e-02, -1.0619e-02,  2.8985e-02, -1.1432e-02,\n",
       "          1.2178e-02,  2.0143e-02,  3.1030e-03,  4.2178e-03,  5.3580e-03],\n",
       "        [ 2.6719e-03, -9.9180e-03,  3.4079e-02, -6.2542e-03,  4.6740e-03,\n",
       "         -4.7698e-03, -1.1945e-03, -2.1102e-03,  7.3434e-04, -1.2093e-02],\n",
       "        [ 1.5972e-03,  1.6520e-02, -1.1650e-02,  3.4791e-02, -1.3646e-02,\n",
       "          1.4544e-02,  2.4287e-02,  3.6749e-03,  5.1194e-03,  6.0543e-03],\n",
       "        [-2.8978e-03, -3.4967e-03, -1.6629e-02, -1.7680e-02,  5.3689e-03,\n",
       "         -5.8718e-03, -1.4578e-02, -8.4562e-04, -3.7682e-03,  4.6815e-03],\n",
       "        [ 2.2472e-03,  1.4847e-02, -4.4143e-03,  3.4539e-02, -1.3051e-02,\n",
       "          1.3957e-02,  2.4820e-02,  3.3243e-03,  5.4523e-03,  3.5500e-03],\n",
       "        [-3.0229e-03, -2.2650e-03, -1.9320e-02, -1.6071e-02,  4.5882e-03,\n",
       "         -5.0540e-03, -1.3668e-02, -5.7809e-04, -3.6426e-03,  5.7019e-03],\n",
       "        [ 2.9925e-03, -9.9761e-03,  3.6554e-02, -5.0616e-03,  4.4058e-03,\n",
       "         -4.4649e-03, -7.7006e-05, -2.1145e-03,  1.0585e-03, -1.2874e-02],\n",
       "        [ 2.7512e-03, -1.0362e-02,  3.5305e-02, -6.6974e-03,  4.9226e-03,\n",
       "         -5.0276e-03, -1.3971e-03, -2.2058e-03,  7.2485e-04, -1.2541e-02],\n",
       "        [ 3.1968e-03, -1.4594e-02,  4.4665e-02, -1.2165e-02,  7.5900e-03,\n",
       "         -7.8207e-03, -4.4672e-03, -3.1246e-03,  3.0986e-04, -1.6083e-02]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dli_out =  dloss @ li2.weight\n",
    "dli_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(li_out.grad, dli_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0240,  0.0347, -0.0587],\n",
       "        [ 0.0299, -0.0671,  0.0373],\n",
       "        [-0.0646,  0.0288,  0.0357],\n",
       "        [ 0.0339, -0.0801,  0.0462],\n",
       "        [ 0.0240,  0.0305, -0.0545],\n",
       "        [ 0.0206, -0.0763,  0.0557],\n",
       "        [ 0.0295,  0.0259, -0.0554],\n",
       "        [-0.0686,  0.0274,  0.0413],\n",
       "        [-0.0670,  0.0303,  0.0366],\n",
       "        [-0.0860,  0.0463,  0.0397]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw1 = dli_out.T @ x\n",
    "dw1, li.weight.grad\n",
    "torch.allclose(dw1, li.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1 = dli_out.sum(0)\n",
    "torch.allclose(db1, li.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "print(dli_out.shape, li.weight.shape)\n",
    "dx = dli_out @ li.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.0510e-05, -3.7117e-05,  1.1785e-02,  7.2215e-03, -2.1346e-03],\n",
       "         [-4.9092e-03, -1.2360e-02, -1.1880e-02,  1.4466e-04,  3.0314e-03],\n",
       "         [ 5.4787e-03,  1.3741e-02, -2.3019e-03, -9.6354e-03, -5.5749e-04],\n",
       "         [-5.7404e-03, -1.4454e-02, -1.4428e-02, -1.5749e-04,  3.6420e-03],\n",
       "         [-2.1783e-04, -5.1056e-04,  1.0783e-02,  6.8930e-03, -1.9194e-03],\n",
       "         [-4.7046e-03, -1.1858e-02, -1.5418e-02, -2.3170e-03,  3.6374e-03],\n",
       "         [-7.8253e-04, -1.9288e-03,  1.0445e-02,  7.5358e-03, -1.7574e-03],\n",
       "         [ 5.6815e-03,  1.4247e-02, -3.2364e-03, -1.0509e-02, -4.2391e-04],\n",
       "         [ 5.7016e-03,  1.4301e-02, -2.2830e-03, -9.9589e-03, -6.0061e-04],\n",
       "         [ 7.6506e-03,  1.9195e-02, -1.1681e-03, -1.2209e-02, -1.1501e-03]]),\n",
       " tensor([[-3.0510e-05, -3.7116e-05,  1.1785e-02,  7.2215e-03, -2.1346e-03],\n",
       "         [-4.9092e-03, -1.2360e-02, -1.1880e-02,  1.4466e-04,  3.0314e-03],\n",
       "         [ 5.4787e-03,  1.3741e-02, -2.3019e-03, -9.6354e-03, -5.5749e-04],\n",
       "         [-5.7404e-03, -1.4454e-02, -1.4428e-02, -1.5749e-04,  3.6420e-03],\n",
       "         [-2.1783e-04, -5.1056e-04,  1.0783e-02,  6.8930e-03, -1.9194e-03],\n",
       "         [-4.7046e-03, -1.1858e-02, -1.5418e-02, -2.3170e-03,  3.6374e-03],\n",
       "         [-7.8253e-04, -1.9288e-03,  1.0445e-02,  7.5358e-03, -1.7574e-03],\n",
       "         [ 5.6815e-03,  1.4247e-02, -3.2364e-03, -1.0509e-02, -4.2391e-04],\n",
       "         [ 5.7016e-03,  1.4301e-02, -2.2830e-03, -9.9589e-03, -6.0061e-04],\n",
       "         [ 7.6506e-03,  1.9195e-02, -1.1681e-03, -1.2209e-02, -1.1501e-03]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,\n",
    "                 fan_in: int,\n",
    "                 fan_out: int,\n",
    "                 bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in ** 0.5\n",
    "        self.bias = torch.randn(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self,\n",
    "                 X: torch.Tensor):\n",
    "        self.last_input = X\n",
    "        self.out = X @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, d_L_d_out):\n",
    "        # d_L_d_weights = torch.matmul(self.last_input.t(), d_L_d_out)\n",
    "\n",
    "        d_L_d_weights = self.last_input.T @ d_L_d_out\n",
    "        d_L_d_biases = torch.sum(d_L_d_out, dim=0)\n",
    "        d_L_d_input = d_L_d_out @ self.weight.T\n",
    "\n",
    "        return d_L_d_input, d_L_d_weights, d_L_d_biases\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_maxes = out.max(1, keepdim=True).values\n",
    "norm_logits = out - logit_maxes #subtract the max for numerical stability refer the previous notebooks \n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims = True)\n",
    "counts_sum_inv = counts_sum ** -1  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(10), y].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1548, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2092],\n",
       "         [0.5867],\n",
       "         [0.1870],\n",
       "         [0.0802],\n",
       "         [0.2657],\n",
       "         [0.2117],\n",
       "         [0.1744],\n",
       "         [0.3677],\n",
       "         [0.2169],\n",
       "         [0.2833]], grad_fn=<MaxBackward0>),\n",
       " tensor([[-0.5269,  0.0000, -0.3183],\n",
       "         [ 0.0000, -1.0006, -0.8468],\n",
       "         [-0.0870, -0.0782,  0.0000],\n",
       "         [-0.1711,  0.0000, -0.2601],\n",
       "         [-0.0527,  0.0000, -0.5498],\n",
       "         [-0.1575, -0.1237,  0.0000],\n",
       "         [ 0.0000, -0.2957, -0.2016],\n",
       "         [ 0.0000, -0.2819, -0.6581],\n",
       "         [-0.0386, -0.0865,  0.0000],\n",
       "         [ 0.0000, -0.1860, -0.6365]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_maxes, norm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5904, 1.0000, 0.7274],\n",
       "        [1.0000, 0.3677, 0.4288],\n",
       "        [0.9166, 0.9248, 1.0000],\n",
       "        [0.8427, 1.0000, 0.7709],\n",
       "        [0.9487, 1.0000, 0.5770],\n",
       "        [0.8543, 0.8836, 1.0000],\n",
       "        [1.0000, 0.7440, 0.8174],\n",
       "        [1.0000, 0.7543, 0.5178],\n",
       "        [0.9622, 0.9171, 1.0000],\n",
       "        [1.0000, 0.8302, 0.5291]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
