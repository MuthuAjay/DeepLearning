{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 1, 2, 1, 2, 0, 0, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((10,5), requires_grad=True)\n",
    "y = torch.randint(0,3, (10,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = nn.Linear(in_features=5, out_features=10)\n",
    "li2 = nn.Linear(10,3)\n",
    "\n",
    "li_out = li(x)\n",
    "out = li2(li_out)\n",
    "out.retain_grad()\n",
    "x.retain_grad()\n",
    "li_out.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "li2.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1918, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(out, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5113, 1.1901, 1.0210,    nan, 0.6620, 2.4949, 0.3594, 1.0085, 0.6821,\n",
       "           nan], grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(out[range(10), y])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1918, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CrossEntropyLoss:\n",
    "\n",
    "    def __call__(self,\n",
    "                 y_pred: torch.Tensor,\n",
    "                 y_true: torch.Tensor\n",
    "                 ):\n",
    "        n_samples = y_pred.shape[0]\n",
    "        y_preds = y_pred - y_pred.max(1, keepdim=True).values\n",
    "        counts = y_preds.exp()\n",
    "        counts_sum = counts.sum(1, keepdim=True)\n",
    "        counts_sum_inv = counts_sum ** -1\n",
    "        probs = counts * counts_sum_inv\n",
    "        logprobs = probs.log()\n",
    "        self.out = -logprobs[range(n_samples), y_true].mean()\n",
    "        return self.out\n",
    "\n",
    "    def backward(self,\n",
    "                 y_pred: torch.Tensor,\n",
    "                 y_true: torch.Tensor\n",
    "                 ):\n",
    "        n_samples = y_pred.shape[0]\n",
    "        # softmax = F.softmax()\n",
    "        grad = F.softmax(y_pred, dim=1)\n",
    "        grad[range(n_samples), y_true] -= 1\n",
    "        grad = grad / n_samples\n",
    "        return grad\n",
    "\n",
    "    def paramerters(self):\n",
    "        return []\n",
    "    \n",
    "lo = CrossEntropyLoss()\n",
    "lo(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dlosstorch.Size([10, 3]), out torch.Size([10, 3])\n",
      "tensor([[ 0.0240,  0.0347, -0.0587],\n",
      "        [ 0.0299, -0.0671,  0.0373],\n",
      "        [-0.0646,  0.0288,  0.0357],\n",
      "        [ 0.0339, -0.0801,  0.0462],\n",
      "        [ 0.0240,  0.0305, -0.0545],\n",
      "        [ 0.0206, -0.0763,  0.0557],\n",
      "        [ 0.0295,  0.0259, -0.0554],\n",
      "        [-0.0686,  0.0274,  0.0413],\n",
      "        [-0.0670,  0.0303,  0.0366],\n",
      "        [-0.0860,  0.0463,  0.0397]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dloss = lo.backward(out, y)\n",
    "\n",
    "print(f\"shape of dloss{dloss.shape}, out {out.shape}\")\n",
    "print(dloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1641, -0.0714, -0.0778,  0.0775,  0.0259,  0.0220,  0.0946, -0.0208,\n",
       "          0.0993,  0.0915],\n",
       "        [-0.1298,  0.0790,  0.0594, -0.1384, -0.0275, -0.0181, -0.0883,  0.0483,\n",
       "         -0.1244, -0.0467],\n",
       "        [-0.0343, -0.0076,  0.0184,  0.0609,  0.0015, -0.0039, -0.0063, -0.0275,\n",
       "          0.0250, -0.0448]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li2.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1641, -0.0714, -0.0778,  0.0775,  0.0259,  0.0220,  0.0946, -0.0208,\n",
       "          0.0993,  0.0915],\n",
       "        [-0.1298,  0.0790,  0.0594, -0.1384, -0.0275, -0.0181, -0.0883,  0.0483,\n",
       "         -0.1244, -0.0467],\n",
       "        [-0.0343, -0.0076,  0.0184,  0.0609,  0.0015, -0.0039, -0.0063, -0.0275,\n",
       "          0.0250, -0.0448]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(li_out.shape, dloss.shape)\n",
    "dw = dloss.T @ li_out\n",
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(dw, li2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1244,  0.0004,  0.1240]),\n",
       " tensor([-0.1244,  0.0004,  0.1240], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = dloss.sum(dim = 0)\n",
    "li2.bias.grad, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0885e-03, -4.2290e-03, -1.7008e-02, -1.9706e-02,  6.0901e-03,\n",
       "         -6.6475e-03, -1.6097e-02, -1.0117e-03, -4.1209e-03,  4.6926e-03],\n",
       "        [ 1.2462e-03,  1.3961e-02, -1.0619e-02,  2.8985e-02, -1.1432e-02,\n",
       "          1.2178e-02,  2.0143e-02,  3.1030e-03,  4.2178e-03,  5.3580e-03],\n",
       "        [ 2.6719e-03, -9.9180e-03,  3.4079e-02, -6.2542e-03,  4.6740e-03,\n",
       "         -4.7698e-03, -1.1945e-03, -2.1102e-03,  7.3434e-04, -1.2093e-02],\n",
       "        [ 1.5972e-03,  1.6520e-02, -1.1650e-02,  3.4791e-02, -1.3646e-02,\n",
       "          1.4544e-02,  2.4287e-02,  3.6749e-03,  5.1194e-03,  6.0543e-03],\n",
       "        [-2.8978e-03, -3.4967e-03, -1.6629e-02, -1.7680e-02,  5.3689e-03,\n",
       "         -5.8718e-03, -1.4578e-02, -8.4562e-04, -3.7682e-03,  4.6815e-03],\n",
       "        [ 2.2472e-03,  1.4847e-02, -4.4143e-03,  3.4539e-02, -1.3051e-02,\n",
       "          1.3957e-02,  2.4820e-02,  3.3243e-03,  5.4523e-03,  3.5500e-03],\n",
       "        [-3.0229e-03, -2.2650e-03, -1.9320e-02, -1.6071e-02,  4.5882e-03,\n",
       "         -5.0540e-03, -1.3668e-02, -5.7809e-04, -3.6426e-03,  5.7019e-03],\n",
       "        [ 2.9925e-03, -9.9761e-03,  3.6554e-02, -5.0616e-03,  4.4058e-03,\n",
       "         -4.4649e-03, -7.7006e-05, -2.1145e-03,  1.0585e-03, -1.2874e-02],\n",
       "        [ 2.7512e-03, -1.0362e-02,  3.5305e-02, -6.6974e-03,  4.9226e-03,\n",
       "         -5.0276e-03, -1.3971e-03, -2.2058e-03,  7.2485e-04, -1.2541e-02],\n",
       "        [ 3.1968e-03, -1.4594e-02,  4.4665e-02, -1.2165e-02,  7.5900e-03,\n",
       "         -7.8207e-03, -4.4672e-03, -3.1246e-03,  3.0986e-04, -1.6083e-02]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dli_out =  dloss @ li2.weight\n",
    "dli_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(li_out.grad, dli_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0240,  0.0347, -0.0587],\n",
       "        [ 0.0299, -0.0671,  0.0373],\n",
       "        [-0.0646,  0.0288,  0.0357],\n",
       "        [ 0.0339, -0.0801,  0.0462],\n",
       "        [ 0.0240,  0.0305, -0.0545],\n",
       "        [ 0.0206, -0.0763,  0.0557],\n",
       "        [ 0.0295,  0.0259, -0.0554],\n",
       "        [-0.0686,  0.0274,  0.0413],\n",
       "        [-0.0670,  0.0303,  0.0366],\n",
       "        [-0.0860,  0.0463,  0.0397]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw1 = dli_out.T @ x\n",
    "dw1, li.weight.grad\n",
    "torch.allclose(dw1, li.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1 = dli_out.sum(0)\n",
    "torch.allclose(db1, li.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "print(dli_out.shape, li.weight.shape)\n",
    "dx = dli_out @ li.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.0510e-05, -3.7117e-05,  1.1785e-02,  7.2215e-03, -2.1346e-03],\n",
       "         [-4.9092e-03, -1.2360e-02, -1.1880e-02,  1.4466e-04,  3.0314e-03],\n",
       "         [ 5.4787e-03,  1.3741e-02, -2.3019e-03, -9.6354e-03, -5.5749e-04],\n",
       "         [-5.7404e-03, -1.4454e-02, -1.4428e-02, -1.5749e-04,  3.6420e-03],\n",
       "         [-2.1783e-04, -5.1056e-04,  1.0783e-02,  6.8930e-03, -1.9194e-03],\n",
       "         [-4.7046e-03, -1.1858e-02, -1.5418e-02, -2.3170e-03,  3.6374e-03],\n",
       "         [-7.8253e-04, -1.9288e-03,  1.0445e-02,  7.5358e-03, -1.7574e-03],\n",
       "         [ 5.6815e-03,  1.4247e-02, -3.2364e-03, -1.0509e-02, -4.2391e-04],\n",
       "         [ 5.7016e-03,  1.4301e-02, -2.2830e-03, -9.9589e-03, -6.0061e-04],\n",
       "         [ 7.6506e-03,  1.9195e-02, -1.1681e-03, -1.2209e-02, -1.1501e-03]]),\n",
       " tensor([[-3.0510e-05, -3.7116e-05,  1.1785e-02,  7.2215e-03, -2.1346e-03],\n",
       "         [-4.9092e-03, -1.2360e-02, -1.1880e-02,  1.4466e-04,  3.0314e-03],\n",
       "         [ 5.4787e-03,  1.3741e-02, -2.3019e-03, -9.6354e-03, -5.5749e-04],\n",
       "         [-5.7404e-03, -1.4454e-02, -1.4428e-02, -1.5749e-04,  3.6420e-03],\n",
       "         [-2.1783e-04, -5.1056e-04,  1.0783e-02,  6.8930e-03, -1.9194e-03],\n",
       "         [-4.7046e-03, -1.1858e-02, -1.5418e-02, -2.3170e-03,  3.6374e-03],\n",
       "         [-7.8253e-04, -1.9288e-03,  1.0445e-02,  7.5358e-03, -1.7574e-03],\n",
       "         [ 5.6815e-03,  1.4247e-02, -3.2364e-03, -1.0509e-02, -4.2391e-04],\n",
       "         [ 5.7016e-03,  1.4301e-02, -2.2830e-03, -9.9589e-03, -6.0061e-04],\n",
       "         [ 7.6506e-03,  1.9195e-02, -1.1681e-03, -1.2209e-02, -1.1501e-03]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,\n",
    "                 fan_in: int,\n",
    "                 fan_out: int,\n",
    "                 bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in ** 0.5\n",
    "        self.bias = torch.randn(fan_out) if bias else None\n",
    "\n",
    "    def __call__(self,\n",
    "                 X: torch.Tensor):\n",
    "        self.last_input = X\n",
    "        self.out = X @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, d_L_d_out):\n",
    "        # d_L_d_weights = torch.matmul(self.last_input.t(), d_L_d_out)\n",
    "\n",
    "        d_L_d_weights = self.last_input.T @ d_L_d_out\n",
    "        d_L_d_biases = torch.sum(d_L_d_out, dim=0)\n",
    "        d_L_d_input = d_L_d_out @ self.weight.T\n",
    "\n",
    "        return d_L_d_input, d_L_d_weights, d_L_d_biases\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_maxes = out.max(1, keepdim=True).values\n",
    "norm_logits = out - logit_maxes #subtract the max for numerical stability refer the previous notebooks \n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims = True)\n",
    "counts_sum_inv = counts_sum ** -1  # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(10), y].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1548, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2092],\n",
       "         [0.5867],\n",
       "         [0.1870],\n",
       "         [0.0802],\n",
       "         [0.2657],\n",
       "         [0.2117],\n",
       "         [0.1744],\n",
       "         [0.3677],\n",
       "         [0.2169],\n",
       "         [0.2833]], grad_fn=<MaxBackward0>),\n",
       " tensor([[-0.5269,  0.0000, -0.3183],\n",
       "         [ 0.0000, -1.0006, -0.8468],\n",
       "         [-0.0870, -0.0782,  0.0000],\n",
       "         [-0.1711,  0.0000, -0.2601],\n",
       "         [-0.0527,  0.0000, -0.5498],\n",
       "         [-0.1575, -0.1237,  0.0000],\n",
       "         [ 0.0000, -0.2957, -0.2016],\n",
       "         [ 0.0000, -0.2819, -0.6581],\n",
       "         [-0.0386, -0.0865,  0.0000],\n",
       "         [ 0.0000, -0.1860, -0.6365]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_maxes, norm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5904, 1.0000, 0.7274],\n",
       "        [1.0000, 0.3677, 0.4288],\n",
       "        [0.9166, 0.9248, 1.0000],\n",
       "        [0.8427, 1.0000, 0.7709],\n",
       "        [0.9487, 1.0000, 0.5770],\n",
       "        [0.8543, 0.8836, 1.0000],\n",
       "        [1.0000, 0.7440, 0.8174],\n",
       "        [1.0000, 0.7543, 0.5178],\n",
       "        [0.9622, 0.9171, 1.0000],\n",
       "        [1.0000, 0.8302, 0.5291]], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Non Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.randn((10, 5), requires_grad=True)\n",
    "y = torch.randint(0,3,(10,))\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3346,  0.3682, -0.4057],\n",
      "        [ 0.0695,  0.0743, -0.3219],\n",
      "        [ 0.1473, -0.1134, -0.1617],\n",
      "        [-0.0190,  0.3161, -0.2893],\n",
      "        [ 0.3820, -0.0245, -0.5353],\n",
      "        [ 0.0672,  0.3544, -0.4217],\n",
      "        [ 0.0575,  0.1941, -0.1656],\n",
      "        [ 0.0911,  0.2511, -0.3718],\n",
      "        [ 0.2456, -0.1051, -0.4400],\n",
      "        [ 0.1275,  0.1981, -0.3591]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "li = nn.Linear(5, 10)\n",
    "li2 = nn.Linear(10,3)\n",
    "re = nn.GELU()\n",
    "\n",
    "li_out = li(x)\n",
    "re_out = re(li_out)\n",
    "li2_out = li2(re_out)\n",
    "\n",
    "for op in [li_out, re_out, li2_out]:\n",
    "    op.retain_grad()\n",
    "print(li2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1506, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(li2_out, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1506, grad_fn=<NegBackward0>) tensor([[-0.0747,  0.0511,  0.0236],\n",
      "        [ 0.0373, -0.0625,  0.0252],\n",
      "        [ 0.0399, -0.0692,  0.0293],\n",
      "        [ 0.0316, -0.0558,  0.0241],\n",
      "        [ 0.0484,  0.0322, -0.0807],\n",
      "        [-0.0661,  0.0452,  0.0208],\n",
      "        [ 0.0339, -0.0611,  0.0272],\n",
      "        [ 0.0357, -0.0581,  0.0225],\n",
      "        [ 0.0453, -0.0681,  0.0228],\n",
      "        [ 0.0372,  0.0399, -0.0771]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def manual_loss(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    y_pred_max = y_pred.max(1, keepdim = True).values\n",
    "    y_pred_max = y_pred - y_pred_max\n",
    "    logprobs  = (y_pred_max.exp()/ y_pred_max.exp().sum(1, keepdim =True)).log()\n",
    "    loss = -logprobs[range(0, n_samples), y_true].mean()\n",
    "    return loss\n",
    "\n",
    "def lbackward(y_pred, y_true):\n",
    "    n_samples = y_pred.shape[0]\n",
    "    grad = F.softmax(y_pred, dim = 1)\n",
    "    grad[range(0,n_samples), y_true] -= 1\n",
    "    grad = grad/ n_samples\n",
    "    return grad\n",
    "    \n",
    "    \n",
    "\n",
    "man_l = manual_loss(li2_out, y)\n",
    "dloss = lbackward(li2_out, y)\n",
    "\n",
    "print(man_l, dloss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0747,  0.0511,  0.0236],\n",
       "        [ 0.0373, -0.0625,  0.0252],\n",
       "        [ 0.0399, -0.0692,  0.0293],\n",
       "        [ 0.0316, -0.0558,  0.0241],\n",
       "        [ 0.0484,  0.0322, -0.0807],\n",
       "        [-0.0661,  0.0452,  0.0208],\n",
       "        [ 0.0339, -0.0611,  0.0272],\n",
       "        [ 0.0357, -0.0581,  0.0225],\n",
       "        [ 0.0453, -0.0681,  0.0228],\n",
       "        [ 0.0372,  0.0399, -0.0771]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li2_out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2531, 0.5112, 0.2357],\n",
       "        [0.3730, 0.3748, 0.2522],\n",
       "        [0.3992, 0.3076, 0.2931],\n",
       "        [0.3163, 0.4423, 0.2414],\n",
       "        [0.4841, 0.3224, 0.1934],\n",
       "        [0.3394, 0.4524, 0.2082],\n",
       "        [0.3394, 0.3891, 0.2715],\n",
       "        [0.3568, 0.4187, 0.2246],\n",
       "        [0.4529, 0.3189, 0.2282],\n",
       "        [0.3720, 0.3993, 0.2287]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x, dim:int):\n",
    "    x = x - x.max(dim, keepdim=True).values\n",
    "    out = x.exp()/ x.exp().sum(dim = dim, keepdim =True)\n",
    "    return out\n",
    "\n",
    "softmax(li2_out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2531, 0.5112, 0.2357],\n",
       "        [0.3730, 0.3748, 0.2522],\n",
       "        [0.3992, 0.3076, 0.2931],\n",
       "        [0.3163, 0.4423, 0.2414],\n",
       "        [0.4841, 0.3224, 0.1934],\n",
       "        [0.3394, 0.4524, 0.2082],\n",
       "        [0.3394, 0.3891, 0.2715],\n",
       "        [0.3568, 0.4187, 0.2246],\n",
       "        [0.4529, 0.3189, 0.2282],\n",
       "        [0.3720, 0.3993, 0.2287]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(li2_out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0118,  0.0408,  0.0765, -0.0865,  0.0398, -0.0183,  0.0924,  0.1141,\n",
       "         -0.1180, -0.0372],\n",
       "        [-0.0151, -0.0544, -0.0540,  0.0724,  0.0017,  0.0256, -0.0657, -0.1810,\n",
       "          0.0606,  0.0311],\n",
       "        [ 0.0033,  0.0136, -0.0224,  0.0141, -0.0416, -0.0072, -0.0267,  0.0669,\n",
       "          0.0574,  0.0061]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw2 = dloss.T @ re_out\n",
    "dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0118,  0.0408,  0.0765, -0.0865,  0.0398, -0.0183,  0.0924,  0.1141,\n",
       "         -0.1180, -0.0372],\n",
       "        [-0.0151, -0.0544, -0.0540,  0.0724,  0.0017,  0.0256, -0.0657, -0.1810,\n",
       "          0.0606,  0.0311],\n",
       "        [ 0.0033,  0.0136, -0.0224,  0.0141, -0.0416, -0.0072, -0.0267,  0.0669,\n",
       "          0.0574,  0.0061]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li2.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1686, -0.2063,  0.0377], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2 = dloss.sum(0)\n",
    "db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1686, -0.2063,  0.0377])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li2.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dre_out = dloss @ li2.weight\n",
    "torch.allclose(dre_out, re_out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw1 = dre_out.T @ li_out\n",
    "dw1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5644e-03, -3.0173e-03, -6.0959e-03, -1.6417e-02, -2.8712e-03,\n",
       "         -1.8655e-03,  1.2507e-03, -4.9156e-03,  2.0204e-02,  1.7993e-02],\n",
       "        [ 4.0592e-03, -2.2008e-02,  7.8039e-03, -4.7219e-04,  1.4989e-02,\n",
       "          2.5203e-03,  7.6645e-03,  1.8097e-02, -2.6238e-03, -1.1918e-04],\n",
       "        [ 9.1209e-03, -9.5669e-03,  9.3411e-03,  6.0112e-04,  4.2643e-03,\n",
       "          1.7493e-03,  9.0313e-03,  2.1762e-02, -1.1307e-02, -3.0412e-05],\n",
       "        [-6.3458e-04, -1.2319e-03,  5.5007e-03,  5.2362e-03,  5.8433e-03,\n",
       "          5.1021e-04,  1.7442e-03,  8.5731e-03, -1.9086e-02,  4.3253e-04],\n",
       "        [-2.1148e-02, -5.0481e-05,  2.6310e-02,  2.7916e-03,  1.8492e-02,\n",
       "          2.5541e-03,  1.8069e-02,  1.8258e-03,  1.3345e-03, -2.1255e-02],\n",
       "        [ 1.5732e-04, -2.4007e-03, -1.6478e-02, -1.2023e-02, -7.6808e-03,\n",
       "         -1.6502e-03, -9.3241e-03, -2.1535e-03,  1.6880e-02,  8.2291e-03],\n",
       "        [ 1.6962e-03, -9.5929e-03,  6.6492e-03,  2.3927e-03,  5.4254e-03,\n",
       "          8.7450e-04,  2.9921e-03,  1.7860e-02, -1.6602e-02,  5.0203e-04],\n",
       "        [ 2.5354e-03,  2.0686e-03,  9.6560e-03,  4.8965e-03,  3.8674e-04,\n",
       "          1.0770e-03,  4.0274e-03,  1.4394e-02, -2.0509e-02, -7.4324e-05],\n",
       "        [ 8.5881e-03, -1.4272e-02,  1.5104e-02,  3.8793e-04,  1.2259e-02,\n",
       "          2.1737e-03,  1.2230e-02,  1.4019e-02, -3.3376e-03,  4.2838e-07],\n",
       "        [ 2.8644e-04, -2.7610e-04,  3.1765e-03,  4.8203e-03,  5.3547e-03,\n",
       "          3.1081e-03,  1.4511e-02, -2.0580e-02,  1.1786e-02, -2.0144e-03]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li_out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0016, -0.0471, -0.0307, -0.0382, -0.0311],\n",
       "        [ 0.0360,  0.0106,  0.0528,  0.0267, -0.0328],\n",
       "        [-0.0655, -0.0119, -0.0209,  0.0398,  0.0311],\n",
       "        [-0.0345, -0.0274, -0.0199,  0.0279, -0.0071],\n",
       "        [-0.0468,  0.0008, -0.0265,  0.0191,  0.0481],\n",
       "        [-0.0068, -0.0082, -0.0030,  0.0016,  0.0062],\n",
       "        [-0.0172, -0.0224,  0.0040,  0.0031,  0.0364],\n",
       "        [-0.0846, -0.0041, -0.0890, -0.0092, -0.0094],\n",
       "        [ 0.0709,  0.0113,  0.0724, -0.0117,  0.0346],\n",
       "        [ 0.0581,  0.0125,  0.0067, -0.0736, -0.0160]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0031, -0.0603,  0.0610, -0.0078,  0.0565,  0.0111,  0.0622,  0.0689,\n",
       "        -0.0233,  0.0037])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0871, -0.1119, -0.1381,  0.6630, -0.1627, -0.1595, -0.1665,  0.2566,\n",
       "          1.6397,  0.4571],\n",
       "        [-0.0831,  0.8619,  0.0823, -0.1654,  0.3347, -0.0838,  0.2873,  0.7110,\n",
       "         -0.1560,  0.0233],\n",
       "        [ 0.1213, -0.0403,  0.1530, -0.1654, -0.1285, -0.1293,  0.4315,  1.1305,\n",
       "         -0.0093, -0.1575],\n",
       "        [-0.1657, -0.1653,  0.0147,  0.2009, -0.0602, -0.1633, -0.1286,  0.0303,\n",
       "          0.6222,  0.3229],\n",
       "        [ 0.0151,  0.1042,  0.7081, -0.1081,  0.4864, -0.1419,  0.3370, -0.1576,\n",
       "         -0.1525,  0.1868],\n",
       "        [-0.1698, -0.1404,  0.0699,  0.3278, -0.1157, -0.1595, -0.0402, -0.0595,\n",
       "          0.7224, -0.0090],\n",
       "        [-0.1542, -0.0031,  0.0649, -0.0948, -0.0856, -0.1539, -0.0726,  0.6602,\n",
       "          0.2684,  0.0561],\n",
       "        [-0.1235, -0.1461,  0.2469,  0.0871, -0.1694, -0.1499, -0.0322,  0.4044,\n",
       "          0.7016, -0.1574],\n",
       "        [ 0.2839,  0.0905,  0.4717, -0.1684,  0.0582, -0.1258,  0.6478,  0.2361,\n",
       "         -0.1516, -0.1700],\n",
       "        [-0.1700, -0.1598, -0.1491,  0.1099, -0.0713, -0.1121,  0.3063,  0.4514,\n",
       "          0.5058, -0.1642]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "import math\n",
    "\n",
    "class GELU:\n",
    "    \n",
    "    def __call__(self, x:torch.Tensor) -> Any:\n",
    "        self.out = 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * torch.pow(x,3))))        \n",
    "        return self.out\n",
    "    \n",
    "gel = GELU()\n",
    "gel_out = gel(li_out)\n",
    "\n",
    "gel_out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0869, -0.1117, -0.1380,  0.6631, -0.1626, -0.1595, -0.1664,  0.2566,\n",
       "          1.6398,  0.4571],\n",
       "        [-0.0831,  0.8620,  0.0823, -0.1653,  0.3347, -0.0838,  0.2873,  0.7111,\n",
       "         -0.1560,  0.0233],\n",
       "        [ 0.1213, -0.0403,  0.1530, -0.1654, -0.1285, -0.1293,  0.4315,  1.1307,\n",
       "         -0.0093, -0.1573],\n",
       "        [-0.1656, -0.1652,  0.0147,  0.2009, -0.0602, -0.1633, -0.1286,  0.0303,\n",
       "          0.6223,  0.3229],\n",
       "        [ 0.0151,  0.1042,  0.7083, -0.1081,  0.4864, -0.1419,  0.3370, -0.1575,\n",
       "         -0.1525,  0.1868],\n",
       "        [-0.1698, -0.1402,  0.0699,  0.3278, -0.1157, -0.1595, -0.0402, -0.0595,\n",
       "          0.7225, -0.0090],\n",
       "        [-0.1542, -0.0031,  0.0649, -0.0948, -0.0856, -0.1539, -0.0726,  0.6603,\n",
       "          0.2684,  0.0561],\n",
       "        [-0.1235, -0.1459,  0.2469,  0.0871, -0.1694, -0.1499, -0.0322,  0.4044,\n",
       "          0.7017, -0.1574],\n",
       "        [ 0.2839,  0.0905,  0.4717, -0.1684,  0.0582, -0.1258,  0.6478,  0.2361,\n",
       "         -0.1515, -0.1700],\n",
       "        [-0.1699, -0.1597, -0.1491,  0.1099, -0.0713, -0.1121,  0.3063,  0.4514,\n",
       "          0.5058, -0.1642]], grad_fn=<GeluBackward0>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gel_out, re_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0869, -0.1117, -0.1380,  0.6631, -0.1626, -0.1595, -0.1664,  0.2566,\n",
       "          1.6398,  0.4571],\n",
       "        [-0.0831,  0.8620,  0.0823, -0.1653,  0.3347, -0.0838,  0.2873,  0.7111,\n",
       "         -0.1560,  0.0233],\n",
       "        [ 0.1213, -0.0403,  0.1530, -0.1654, -0.1285, -0.1293,  0.4315,  1.1307,\n",
       "         -0.0093, -0.1573],\n",
       "        [-0.1656, -0.1652,  0.0147,  0.2009, -0.0602, -0.1633, -0.1286,  0.0303,\n",
       "          0.6223,  0.3229],\n",
       "        [ 0.0151,  0.1042,  0.7083, -0.1081,  0.4864, -0.1419,  0.3370, -0.1575,\n",
       "         -0.1525,  0.1868],\n",
       "        [-0.1698, -0.1402,  0.0699,  0.3278, -0.1157, -0.1595, -0.0402, -0.0595,\n",
       "          0.7225, -0.0090],\n",
       "        [-0.1542, -0.0031,  0.0649, -0.0948, -0.0856, -0.1539, -0.0726,  0.6603,\n",
       "          0.2684,  0.0561],\n",
       "        [-0.1235, -0.1459,  0.2469,  0.0871, -0.1694, -0.1499, -0.0322,  0.4044,\n",
       "          0.7017, -0.1574],\n",
       "        [ 0.2839,  0.0905,  0.4717, -0.1684,  0.0582, -0.1258,  0.6478,  0.2361,\n",
       "         -0.1515, -0.1700],\n",
       "        [-0.1699, -0.1597, -0.1491,  0.1099, -0.0713, -0.1121,  0.3063,  0.4514,\n",
       "          0.5058, -0.1642]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gelu(x):\n",
    "    cdf = 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "    return x * cdf\n",
    "\n",
    "gelu(li_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gelu(li_out), re_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
