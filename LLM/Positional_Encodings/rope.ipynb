{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension of the embedding.\n",
    "        end: Maximum sequence length.\n",
    "        theta: Base value for the frequency calculation.\n",
    "    \n",
    "    Returns:\n",
    "        Complex tensor with shape [end, dim // 2] for efficient computation.\n",
    "    \"\"\"\n",
    "    # Ensure dim is even\n",
    "    if dim % 2 != 0:\n",
    "        raise ValueError(f\"Dimension {dim} must be even\")\n",
    "    \n",
    "    # Create frequencies for each dimension\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # Create position indices\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    \n",
    "    # Outer product of position indices and frequencies\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    \n",
    "    # Compute complex exponentials: cos(x) + i*sin(x)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    \n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "    \n",
    "    Args:\n",
    "        xq: Query states tensor of shape [batch_size, seq_len, n_heads, head_dim]\n",
    "        xk: Key states tensor of shape [batch_size, seq_len, n_heads, head_dim]\n",
    "        freqs_cis: Complex tensor of shape [seq_len, head_dim/2]\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (xq_out, xk_out) with the same shape as the input tensors.\n",
    "    \"\"\"\n",
    "    # Extract shapes\n",
    "    batch, seq_len, n_heads, head_dim = xq.shape\n",
    "    \n",
    "    # Ensure head_dim is even\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"Head dimension {head_dim} must be even\")\n",
    "    \n",
    "    # Reshape inputs to complex-valued tensors\n",
    "    xq_complex = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_complex = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    # Extend frequency tensor to match the batch and heads dimensions\n",
    "    freqs_cis = freqs_cis[:seq_len]\n",
    "    \n",
    "    # Apply rotation using complex multiplication\n",
    "    xq_out = torch.view_as_real(xq_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).flatten(-2)\n",
    "    xk_out = torch.view_as_real(xk_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).flatten(-2)\n",
    "    \n",
    "    # Return the rotated tensors with original dtype\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary positional embedding implementation as a PyTorch module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        self.register_buffer(\n",
    "            \"freqs_cis\", precompute_freqs_cis(self.dim, self.max_seq_len, self.base)\n",
    "        )\n",
    "        \n",
    "    def forward(self, q, k):\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to query and key tensors.\n",
    "        \n",
    "        Args:\n",
    "            q: Query tensor\n",
    "            k: Key tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (q, k) with rotary embeddings applied\n",
    "        \"\"\"\n",
    "        return apply_rotary_emb(q, k, self.freqs_cis)\n",
    "\n",
    "\n",
    "# Example usage in a self-attention layer\n",
    "class SelfAttentionWithRoPE(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Rotary embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask=None) -> torch.Tensor:\n",
    "        batch_size, seq_length = hidden_states.shape[:2]\n",
    "        \n",
    "        # Project to query, key, value\n",
    "        q:torch.Tensor = self.q_proj(hidden_states)\n",
    "        k:torch.Tensor = self.k_proj(hidden_states)\n",
    "        v:torch.Tensor = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        q, k = self.rotary_emb(q, k)\n",
    "        \n",
    "        # Transpose for attention calculation\n",
    "        q = q.transpose(1, 2)  # (batch, num_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Get weighted sum\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Transpose and reshape\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_length, self.hidden_size\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Model parameters\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Create random input\n",
    "    hidden_states = torch.rand(batch_size, seq_length, hidden_size)\n",
    "    \n",
    "    # Initialize the self-attention layer with RoPE\n",
    "    self_attn = SelfAttentionWithRoPE(hidden_size, num_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = self_attn(hidden_states)\n",
    "    print(f\"Input shape: {hidden_states.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # We can verify the output shape matches the input shape\n",
    "    assert output.shape == hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension of the embedding.\n",
    "        end: Maximum sequence length.\n",
    "        theta: Base value for the frequency calculation.\n",
    "    \n",
    "    Returns:\n",
    "        Complex tensor with shape [end, dim // 2] for efficient computation.\n",
    "    \"\"\"\n",
    "    # Ensure dim is even\n",
    "    if dim % 2 != 0:\n",
    "        raise ValueError(f\"Dimension {dim} must be even\")\n",
    "    \n",
    "    # Create frequencies for each dimension\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # Create position indices\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    \n",
    "    # Outer product of position indices and frequencies\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    \n",
    "    # Compute complex exponentials: cos(x) + i*sin(x)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    \n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_length = 10\n",
    "hidden_size = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Create random input\n",
    "hidden_states = torch.rand(batch_size, seq_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7071+0.7071j])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "magnitude = torch.tensor([1.0])  # abs = 1\n",
    "angle = torch.tensor([torch.pi / 4])  # angle = π/4\n",
    "\n",
    "complex_number = torch.polar(magnitude, angle)\n",
    "print(complex_number)  # Output: tensor([0.7071+0.7071j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00+0.0000e+00j, -4.3711e-08+1.0000e+00j],\n",
      "        [-1.0000e+00-8.7423e-08j,  1.1925e-08-1.0000e+00j]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define frequency matrix\n",
    "freqs = torch.tensor([[0.0, torch.pi / 2], [torch.pi, 3*torch.pi / 2]])  # Angles\n",
    "magnitudes = torch.ones_like(freqs)  # Unit circle (magnitude = 1)\n",
    "\n",
    "# Compute complex exponentials\n",
    "freqs_cis = torch.polar(magnitudes, freqs)\n",
    "\n",
    "print(freqs_cis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7071, 0.7071]])\n",
      "tensor([0.7071+0.7071j])\n"
     ]
    }
   ],
   "source": [
    "def custom_polar(magnitude, angle):\n",
    "    real = magnitude * torch.cos(angle)\n",
    "    imag = magnitude * torch.sin(angle)\n",
    "    \n",
    "    return torch.stack([real, imag], dim=-1)\n",
    "\n",
    "magnitude = torch.tensor([1.0])  # abs = 1\n",
    "angle = torch.tensor([torch.pi / 4])  # angle = π/4\n",
    "\n",
    "complex_number = custom_polar(magnitude, angle)\n",
    "\n",
    "\n",
    "print(complex_number)  # Output: tensor([0.7071, 0.7071])\n",
    "\n",
    "print(torch.view_as_complex(complex_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 512\n",
    "theta = 10000.0\n",
    "\n",
    "if dim % 2 != 0:\n",
    "    raise ValueError(f\"Dimension {dim} must be even\")\n",
    "\n",
    "# Create frequencies for each dimension\n",
    "freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "\n",
    "freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7737e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7737e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 / (10000 ** (torch.arange(0, dim, 2)[: (dim // 2)].float()/ dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7737e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7737e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 10\n",
    "\n",
    "t = torch.arange(seq_length)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.outer(t, freqs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 2.,  4.,  6.],\n",
       "        [ 3.,  6.,  9.],\n",
       "        [ 4.,  8., 12.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.arange(1., 5.)\n",
    "v2 = torch.arange(1., 4.)\n",
    "torch.outer(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.],\n",
       "        [ 2.,  4.,  6.,  8.],\n",
       "        [ 3.,  6.,  9., 12.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.outer(v1, v2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4.]), tensor([1., 2., 3.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 2.,  4.,  6.],\n",
       "        [ 3.,  6.,  9.],\n",
       "        [ 4.,  8., 12.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(v1.unsqueeze(-1), v2.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4.]), tensor([1., 2., 3., 4.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 , v1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.]]),\n",
       " tensor([[1., 2., 3.]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.unsqueeze(-1), v2.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.outer is works only on 1D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(torch.outer(t, freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.unsqueeze(1).shape, freqs.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "         8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "         6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "         5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "         4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "         3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "         2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "         2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "         1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "         1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "         1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "         9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7737e-02,\n",
       "         7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "         6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "         4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "         3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "         3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "         2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "         2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "         1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "         1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "         1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "         8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7737e-03, 7.4989e-03, 7.2339e-03,\n",
       "         6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "         5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "         4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "         3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "         2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "         2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "         1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "         1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "         1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "         1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "         8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "         6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "         5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "         4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "         3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "         2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "         2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "         1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "         1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "         1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [8],\n",
       "        [9]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 9.6466e-01, 9.3057e-01,  ..., 1.1140e-04, 1.0746e-04,\n",
       "         1.0366e-04],\n",
       "        [2.0000e+00, 1.9293e+00, 1.8611e+00,  ..., 2.2279e-04, 2.1492e-04,\n",
       "         2.0733e-04],\n",
       "        ...,\n",
       "        [7.0000e+00, 6.7526e+00, 6.5140e+00,  ..., 7.7978e-04, 7.5223e-04,\n",
       "         7.2564e-04],\n",
       "        [8.0000e+00, 7.7173e+00, 7.4446e+00,  ..., 8.9118e-04, 8.5969e-04,\n",
       "         8.2931e-04],\n",
       "        [9.0000e+00, 8.6820e+00, 8.3751e+00,  ..., 1.0026e-03, 9.6715e-04,\n",
       "         9.3297e-04]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.outer(t, freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
