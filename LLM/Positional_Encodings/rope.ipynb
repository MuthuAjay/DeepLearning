{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 8, 64])\n",
      "torch.Size([2, 10, 8, 64])\n",
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension of the embedding.\n",
    "        end: Maximum sequence length.\n",
    "        theta: Base value for the frequency calculation.\n",
    "    \n",
    "    Returns:\n",
    "        Complex tensor with shape [end, dim // 2] for efficient computation.\n",
    "    \"\"\"\n",
    "    # Ensure dim is even\n",
    "    if dim % 2 != 0:\n",
    "        raise ValueError(f\"Dimension {dim} must be even\")\n",
    "    \n",
    "    # Create frequencies for each dimension\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # Create position indices\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    \n",
    "    # Outer product of position indices and frequencies\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    \n",
    "    # Compute complex exponentials: cos(x) + i*sin(x)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    \n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "    \n",
    "    Args:\n",
    "        xq: Query states tensor of shape [batch_size, seq_len, n_heads, head_dim]\n",
    "        xk: Key states tensor of shape [batch_size, seq_len, n_heads, head_dim]\n",
    "        freqs_cis: Complex tensor of shape [seq_len, head_dim/2]\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (xq_out, xk_out) with the same shape as the input tensors.\n",
    "    \"\"\"\n",
    "    # Extract shapes\n",
    "    batch, seq_len, n_heads, head_dim = xq.shape\n",
    "    \n",
    "    # Ensure head_dim is even\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"Head dimension {head_dim} must be even\")\n",
    "    \n",
    "    # Reshape inputs to complex-valued tensors\n",
    "    xq_complex = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_complex = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    # Extend frequency tensor to match the batch and heads dimensions\n",
    "    freqs_cis = freqs_cis[:seq_len]\n",
    "    \n",
    "    # Apply rotation using complex multiplication\n",
    "    xq_out = torch.view_as_real(xq_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).flatten(-2)\n",
    "    xk_out = torch.view_as_real(xk_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).flatten(-2)\n",
    "    \n",
    "    # Return the rotated tensors with original dtype\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary positional embedding implementation as a PyTorch module.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.base = base\n",
    "        self.register_buffer(\n",
    "            \"freqs_cis\", precompute_freqs_cis(self.dim, self.max_seq_len, self.base)\n",
    "        )\n",
    "        \n",
    "    def forward(self, q, k):\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to query and key tensors.\n",
    "        \n",
    "        Args:\n",
    "            q: Query tensor\n",
    "            k: Key tensor\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (q, k) with rotary embeddings applied\n",
    "        \"\"\"\n",
    "        return apply_rotary_emb(q, k, self.freqs_cis)\n",
    "\n",
    "\n",
    "# Example usage in a self-attention layer\n",
    "class SelfAttentionWithRoPE(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Rotary embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_seq_len)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, attention_mask=None) -> torch.Tensor:\n",
    "        batch_size, seq_length = hidden_states.shape[:2]\n",
    "        \n",
    "        # Project to query, key, value\n",
    "        q:torch.Tensor = self.q_proj(hidden_states)\n",
    "        k:torch.Tensor = self.k_proj(hidden_states)\n",
    "        v:torch.Tensor = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        k = k.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        \n",
    "        print(q.shape)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        q, k = self.rotary_emb(q, k)\n",
    "        \n",
    "        print(q.shape)\n",
    "        \n",
    "        # Transpose for attention calculation\n",
    "        q = q.transpose(1, 2)  # (batch, num_heads, seq_len, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        # Get weighted sum\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Transpose and reshape\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_length, self.hidden_size\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Model parameters\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "    hidden_size = 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Create random input\n",
    "    hidden_states = torch.rand(batch_size, seq_length, hidden_size)\n",
    "    \n",
    "    # Initialize the self-attention layer with RoPE\n",
    "    self_attn = SelfAttentionWithRoPE(hidden_size, num_heads)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = self_attn(hidden_states)\n",
    "    print(f\"Input shape: {hidden_states.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    \n",
    "    # We can verify the output shape matches the input shape\n",
    "    assert output.shape == hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension of the embedding.\n",
    "        end: Maximum sequence length.\n",
    "        theta: Base value for the frequency calculation.\n",
    "    \n",
    "    Returns:\n",
    "        Complex tensor with shape [end, dim // 2] for efficient computation.\n",
    "    \"\"\"\n",
    "    # Ensure dim is even\n",
    "    if dim % 2 != 0:\n",
    "        raise ValueError(f\"Dimension {dim} must be even\")\n",
    "    \n",
    "    # Create frequencies for each dimension\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # Create position indices\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    \n",
    "    # Outer product of position indices and frequencies\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    \n",
    "    # Compute complex exponentials: cos(x) + i*sin(x)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    \n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "hidden_size = 512\n",
    "\n",
    "num_heads = 8\n",
    "\n",
    "# Create random input\n",
    "hidden_states = torch.rand(batch_size, seq_length, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "magnitude = torch.tensor([1.0])  # abs = 1\n",
    "angle = torch.tensor([torch.pi / 4])  # angle = π/4\n",
    "\n",
    "complex_number = torch.polar(magnitude, angle)\n",
    "print(complex_number)  # Output: tensor([0.7071+0.7071j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define frequency matrix\n",
    "freqs = torch.tensor([[0.0, torch.pi / 2], [torch.pi, 3*torch.pi / 2]])  # Angles\n",
    "magnitudes = torch.ones_like(freqs)  # Unit circle (magnitude = 1)\n",
    "\n",
    "# Compute complex exponentials\n",
    "freqs_cis = torch.polar(magnitudes, freqs)\n",
    "\n",
    "print(freqs_cis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_polar(magnitude, angle):\n",
    "    real = magnitude * torch.cos(angle)\n",
    "    imag = magnitude * torch.sin(angle)\n",
    "    \n",
    "    return torch.stack([real, imag], dim=-1)\n",
    "\n",
    "magnitude = torch.tensor([1.0])  # abs = 1\n",
    "angle = torch.tensor([torch.pi / 4])  # angle = π/4\n",
    "\n",
    "complex_number = custom_polar(magnitude, angle)\n",
    "\n",
    "\n",
    "print(complex_number)  # Output: tensor([0.7071, 0.7071])\n",
    "\n",
    "print(torch.view_as_complex(complex_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 512\n",
    "end = 10\n",
    "\n",
    "torch.arange(0, dim, 2)[: (dim // 2)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, dim, 2) == torch.arange(0, dim, 2)[: (dim // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0039, 0.0078, 0.0117, 0.0156, 0.0195, 0.0234, 0.0273, 0.0312,\n",
       "        0.0352, 0.0391, 0.0430, 0.0469, 0.0508, 0.0547, 0.0586, 0.0625, 0.0664,\n",
       "        0.0703, 0.0742, 0.0781, 0.0820, 0.0859, 0.0898, 0.0938, 0.0977, 0.1016,\n",
       "        0.1055, 0.1094, 0.1133, 0.1172, 0.1211, 0.1250, 0.1289, 0.1328, 0.1367,\n",
       "        0.1406, 0.1445, 0.1484, 0.1523, 0.1562, 0.1602, 0.1641, 0.1680, 0.1719,\n",
       "        0.1758, 0.1797, 0.1836, 0.1875, 0.1914, 0.1953, 0.1992, 0.2031, 0.2070,\n",
       "        0.2109, 0.2148, 0.2188, 0.2227, 0.2266, 0.2305, 0.2344, 0.2383, 0.2422,\n",
       "        0.2461, 0.2500, 0.2539, 0.2578, 0.2617, 0.2656, 0.2695, 0.2734, 0.2773,\n",
       "        0.2812, 0.2852, 0.2891, 0.2930, 0.2969, 0.3008, 0.3047, 0.3086, 0.3125,\n",
       "        0.3164, 0.3203, 0.3242, 0.3281, 0.3320, 0.3359, 0.3398, 0.3438, 0.3477,\n",
       "        0.3516, 0.3555, 0.3594, 0.3633, 0.3672, 0.3711, 0.3750, 0.3789, 0.3828,\n",
       "        0.3867, 0.3906, 0.3945, 0.3984, 0.4023, 0.4062, 0.4102, 0.4141, 0.4180,\n",
       "        0.4219, 0.4258, 0.4297, 0.4336, 0.4375, 0.4414, 0.4453, 0.4492, 0.4531,\n",
       "        0.4570, 0.4609, 0.4648, 0.4688, 0.4727, 0.4766, 0.4805, 0.4844, 0.4883,\n",
       "        0.4922, 0.4961, 0.5000, 0.5039, 0.5078, 0.5117, 0.5156, 0.5195, 0.5234,\n",
       "        0.5273, 0.5312, 0.5352, 0.5391, 0.5430, 0.5469, 0.5508, 0.5547, 0.5586,\n",
       "        0.5625, 0.5664, 0.5703, 0.5742, 0.5781, 0.5820, 0.5859, 0.5898, 0.5938,\n",
       "        0.5977, 0.6016, 0.6055, 0.6094, 0.6133, 0.6172, 0.6211, 0.6250, 0.6289,\n",
       "        0.6328, 0.6367, 0.6406, 0.6445, 0.6484, 0.6523, 0.6562, 0.6602, 0.6641,\n",
       "        0.6680, 0.6719, 0.6758, 0.6797, 0.6836, 0.6875, 0.6914, 0.6953, 0.6992,\n",
       "        0.7031, 0.7070, 0.7109, 0.7148, 0.7188, 0.7227, 0.7266, 0.7305, 0.7344,\n",
       "        0.7383, 0.7422, 0.7461, 0.7500, 0.7539, 0.7578, 0.7617, 0.7656, 0.7695,\n",
       "        0.7734, 0.7773, 0.7812, 0.7852, 0.7891, 0.7930, 0.7969, 0.8008, 0.8047,\n",
       "        0.8086, 0.8125, 0.8164, 0.8203, 0.8242, 0.8281, 0.8320, 0.8359, 0.8398,\n",
       "        0.8438, 0.8477, 0.8516, 0.8555, 0.8594, 0.8633, 0.8672, 0.8711, 0.8750,\n",
       "        0.8789, 0.8828, 0.8867, 0.8906, 0.8945, 0.8984, 0.9023, 0.9062, 0.9102,\n",
       "        0.9141, 0.9180, 0.9219, 0.9258, 0.9297, 0.9336, 0.9375, 0.9414, 0.9453,\n",
       "        0.9492, 0.9531, 0.9570, 0.9609, 0.9648, 0.9688, 0.9727, 0.9766, 0.9805,\n",
       "        0.9844, 0.9883, 0.9922, 0.9961])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, dim, 2)[: (dim // 2)].float()/ dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 512\n",
    "theta = 10000.0\n",
    "\n",
    "if dim % 2 != 0:\n",
    "    raise ValueError(f\"Dimension {dim} must be even\")\n",
    "\n",
    "# Create frequencies for each dimension\n",
    "freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "\n",
    "freqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7737e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7737e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 / (10000 ** (torch.arange(0, dim, 2)[: (dim // 2)].float()/ dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7737e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7737e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 10\n",
    "\n",
    "t = torch.arange(seq_length)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.outer(t, freqs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 2.,  4.,  6.],\n",
       "        [ 3.,  6.,  9.],\n",
       "        [ 4.,  8., 12.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.arange(1., 5.)\n",
    "v2 = torch.arange(1., 4.)\n",
    "torch.outer(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.,  4.],\n",
       "        [ 2.,  4.,  6.,  8.],\n",
       "        [ 3.,  6.,  9., 12.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.outer(v1, v2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4.]), tensor([1., 2., 3.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 2.,  4.,  6.],\n",
       "        [ 3.,  6.,  9.],\n",
       "        [ 4.,  8., 12.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(v1.unsqueeze(-1), v2.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1950/3954854178.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3725.)\n",
      "  v1 , v1.T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4.]), tensor([1., 2., 3., 4.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 , v1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.]]),\n",
       " tensor([[1., 2., 3.]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.unsqueeze(-1), v2.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.outer is works only on 1D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(torch.outer(t, freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1]), torch.Size([1, 256]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.unsqueeze(1).shape, freqs.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "         8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "         6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "         5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "         4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "         3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "         2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "         2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "         1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "         1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "         1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "         9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7737e-02,\n",
       "         7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "         6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "         4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "         3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "         3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "         2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "         2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "         1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "         1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "         1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "         8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7737e-03, 7.4989e-03, 7.2339e-03,\n",
       "         6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "         5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "         4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "         3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "         2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "         2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "         1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "         1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "         1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "         1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "         8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "         6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "         5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "         4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "         3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "         2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "         2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "         1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "         1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "         1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.outer(t, freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    \"\"\"\n",
    "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
    "    \n",
    "    Args:\n",
    "        dim: Dimension of the embedding.\n",
    "        end: Maximum sequence length.\n",
    "        theta: Base value for the frequency calculation.\n",
    "    \n",
    "    Returns:\n",
    "        Complex tensor with shape [end, dim // 2] for efficient computation.\n",
    "    \"\"\"\n",
    "    # Ensure dim is even\n",
    "    if dim % 2 != 0:\n",
    "        raise ValueError(f\"Dimension {dim} must be even\")\n",
    "    \n",
    "    # Create frequencies for each dimension\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # Create position indices\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    \n",
    "    # Outer product of position indices and frequencies\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    \n",
    "    # Compute complex exponentials: cos(x) + i*sin(x)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    \n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0000+0.0000e+00j,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
       "           ...,  1.0000+0.0000e+00j,  1.0000+0.0000e+00j,\n",
       "           1.0000+0.0000e+00j],\n",
       "         [ 0.5403+8.4147e-01j,  0.5697+8.2186e-01j,  0.5974+8.0196e-01j,\n",
       "           ...,  1.0000+1.1140e-04j,  1.0000+1.0746e-04j,\n",
       "           1.0000+1.0366e-04j],\n",
       "         [-0.4161+9.0930e-01j, -0.3509+9.3641e-01j, -0.2863+9.5814e-01j,\n",
       "           ...,  1.0000+2.2279e-04j,  1.0000+2.1492e-04j,\n",
       "           1.0000+2.0733e-04j],\n",
       "         ...,\n",
       "         [ 0.7539+6.5699e-01j,  0.8918+4.5239e-01j,  0.9735+2.2877e-01j,\n",
       "           ...,  1.0000+7.7978e-04j,  1.0000+7.5223e-04j,\n",
       "           1.0000+7.2564e-04j],\n",
       "         [-0.1455+9.8936e-01j,  0.1363+9.9067e-01j,  0.3981+9.1736e-01j,\n",
       "           ...,  1.0000+8.9118e-04j,  1.0000+8.5969e-04j,\n",
       "           1.0000+8.2931e-04j],\n",
       "         [-0.9111+4.1212e-01j, -0.7366+6.7637e-01j, -0.4979+8.6724e-01j,\n",
       "           ...,  1.0000+1.0026e-03j,  1.0000+9.6715e-04j,\n",
       "           1.0000+9.3297e-04j]]),\n",
       " torch.Size([10, 256]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis = precompute_freqs_cis(512, 10)\n",
    "freqs_cis , freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "    \n",
    "    Args:\n",
    "        xq: Query states tensor of shape [batch_size, seq_len, n_heads, head_dim]\n",
    "        xk: Key states tensor of shape [batch_size, seq_len, n_heads, head_dim]\n",
    "        freqs_cis: Complex tensor of shape [seq_len, head_dim/2]\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (xq_out, xk_out) with the same shape as the input tensors.\n",
    "    \"\"\"\n",
    "    # Extract shapes\n",
    "    batch, seq_len, n_heads, head_dim = xq.shape\n",
    "    \n",
    "    # Ensure head_dim is even\n",
    "    if head_dim % 2 != 0:\n",
    "        raise ValueError(f\"Head dimension {head_dim} must be even\")\n",
    "    \n",
    "    # Reshape inputs to complex-valued tensors\n",
    "    xq_complex = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_complex = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    # Extend frequency tensor to match the batch and heads dimensions\n",
    "    freqs_cis = freqs_cis[:seq_len]\n",
    "    \n",
    "    # Apply rotation using complex multiplication\n",
    "    xq_out = torch.view_as_real(xq_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).flatten(-2)\n",
    "    xk_out = torch.view_as_real(xk_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).flatten(-2)\n",
    "    \n",
    "    # Return the rotated tensors with original dtype\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 8, 64]), torch.Size([2, 10, 8, 64]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random states\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "q = torch.rand(2, 10, 8, 64)\n",
    "k = torch.rand(2, 10, 8, 64)\n",
    "\n",
    "freqs_cis = precompute_freqs_cis(64, 10)\n",
    "\n",
    "q_ , k_ = apply_rotary_emb(q, k, freqs_cis)\n",
    "\n",
    "q_.shape, k_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.8823, 0.9150, 0.3829,  ..., 0.1587, 0.6542, 0.3278],\n",
       "           [0.6532, 0.3958, 0.9147,  ..., 0.2083, 0.3289, 0.1054],\n",
       "           [0.9192, 0.4008, 0.9302,  ..., 0.5535, 0.4117, 0.3510],\n",
       "           ...,\n",
       "           [0.1525, 0.3970, 0.8703,  ..., 0.1474, 0.6872, 0.9231],\n",
       "           [0.5070, 0.9549, 0.0740,  ..., 0.2564, 0.1352, 0.9012],\n",
       "           [0.8918, 0.1182, 0.4613,  ..., 0.4078, 0.5411, 0.0410]],\n",
       " \n",
       "          [[0.6556, 0.1186, 0.1836,  ..., 0.6907, 0.9170, 0.3513],\n",
       "           [0.3546, 0.7670, 0.2533,  ..., 0.2422, 0.0622, 0.3856],\n",
       "           [0.6020, 0.0316, 0.9366,  ..., 0.2662, 0.2614, 0.0806],\n",
       "           ...,\n",
       "           [0.0620, 0.2249, 0.1381,  ..., 0.5235, 0.8648, 0.6559],\n",
       "           [0.3225, 0.2944, 0.3762,  ..., 0.3112, 0.9130, 0.5512],\n",
       "           [0.1261, 0.5031, 0.1117,  ..., 0.3092, 0.0702, 0.1836]],\n",
       " \n",
       "          [[0.7785, 0.4253, 0.7124,  ..., 0.8245, 0.9554, 0.7918],\n",
       "           [0.2408, 0.0055, 0.6897,  ..., 0.5963, 0.0773, 0.8968],\n",
       "           [0.6508, 0.5928, 0.2064,  ..., 0.9196, 0.2531, 0.9596],\n",
       "           ...,\n",
       "           [0.5398, 0.2187, 0.2749,  ..., 0.2926, 0.4102, 0.4424],\n",
       "           [0.2240, 0.5240, 0.4298,  ..., 0.9463, 0.5633, 0.6065],\n",
       "           [0.4659, 0.8434, 0.0495,  ..., 0.4593, 0.4520, 0.1866]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.1288, 0.4779, 0.1383,  ..., 0.7426, 0.3062, 0.7291],\n",
       "           [0.8395, 0.7156, 0.7573,  ..., 0.2682, 0.5737, 0.8341],\n",
       "           [0.2907, 0.5922, 0.8981,  ..., 0.8086, 0.1498, 0.8783],\n",
       "           ...,\n",
       "           [0.1457, 0.1499, 0.3298,  ..., 0.9624, 0.6400, 0.7409],\n",
       "           [0.1709, 0.5797, 0.6340,  ..., 0.6885, 0.2405, 0.5956],\n",
       "           [0.9199, 0.1247, 0.3573,  ..., 0.6752, 0.2058, 0.5027]],\n",
       " \n",
       "          [[0.1458, 0.9024, 0.9217,  ..., 0.1868, 0.6352, 0.8431],\n",
       "           [0.9549, 0.4435, 0.6924,  ..., 0.1168, 0.7160, 0.5462],\n",
       "           [0.1616, 0.1054, 0.8614,  ..., 0.4531, 0.4736, 0.9448],\n",
       "           ...,\n",
       "           [0.8079, 0.0307, 0.1572,  ..., 0.0170, 0.3146, 0.5670],\n",
       "           [0.4120, 0.0343, 0.8756,  ..., 0.9512, 0.4349, 0.4081],\n",
       "           [0.8428, 0.5161, 0.9694,  ..., 0.5800, 0.1331, 0.3633]],\n",
       " \n",
       "          [[0.5290, 0.1225, 0.6358,  ..., 0.3148, 0.2628, 0.8908],\n",
       "           [0.7965, 0.6421, 0.4776,  ..., 0.8538, 0.0125, 0.3092],\n",
       "           [0.8508, 0.4516, 0.6684,  ..., 0.0010, 0.0470, 0.7179],\n",
       "           ...,\n",
       "           [0.8891, 0.1301, 0.5359,  ..., 0.1961, 0.0353, 0.7406],\n",
       "           [0.7834, 0.7834, 0.5877,  ..., 0.3160, 0.1067, 0.1089],\n",
       "           [0.2690, 0.5798, 0.4421,  ..., 0.9105, 0.1598, 0.5711]]],\n",
       " \n",
       " \n",
       "         [[[0.3960, 0.0112, 0.0141,  ..., 0.6177, 0.8134, 0.2324],\n",
       "           [0.4402, 0.2020, 0.1635,  ..., 0.1304, 0.9454, 0.5923],\n",
       "           [0.5934, 0.6982, 0.8041,  ..., 0.6219, 0.6251, 0.3298],\n",
       "           ...,\n",
       "           [0.0315, 0.4194, 0.4988,  ..., 0.9535, 0.5774, 0.5616],\n",
       "           [0.5357, 0.3242, 0.3692,  ..., 0.7152, 0.0446, 0.9716],\n",
       "           [0.5689, 0.3157, 0.9461,  ..., 0.4762, 0.5365, 0.9330]],\n",
       " \n",
       "          [[0.7173, 0.3845, 0.0984,  ..., 0.0652, 0.2564, 0.8987],\n",
       "           [0.4030, 0.9034, 0.7961,  ..., 0.3591, 0.6508, 0.6978],\n",
       "           [0.8806, 0.0134, 0.8877,  ..., 0.9800, 0.3773, 0.2866],\n",
       "           ...,\n",
       "           [0.7833, 0.8420, 0.9230,  ..., 0.7663, 0.1121, 0.3626],\n",
       "           [0.5491, 0.6807, 0.2700,  ..., 0.2298, 0.7875, 0.1387],\n",
       "           [0.9174, 0.3991, 0.4932,  ..., 0.0463, 0.1619, 0.3906]],\n",
       " \n",
       "          [[0.1043, 0.5936, 0.4006,  ..., 0.6405, 0.1094, 0.4978],\n",
       "           [0.2302, 0.2166, 0.2073,  ..., 0.7467, 0.8966, 0.4844],\n",
       "           [0.1777, 0.8594, 0.9069,  ..., 0.7675, 0.9807, 0.9457],\n",
       "           ...,\n",
       "           [0.8102, 0.7344, 0.7794,  ..., 0.7813, 0.6398, 0.5022],\n",
       "           [0.9461, 0.4711, 0.6758,  ..., 0.0695, 0.0406, 0.6698],\n",
       "           [0.9694, 0.3171, 0.9710,  ..., 0.4332, 0.6101, 0.1765]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0.0201, 0.3343, 0.2341,  ..., 0.2350, 0.9007, 0.6130],\n",
       "           [0.7766, 0.6715, 0.8157,  ..., 0.0887, 0.3489, 0.9017],\n",
       "           [0.3534, 0.1611, 0.1453,  ..., 0.2690, 0.5844, 0.8620],\n",
       "           ...,\n",
       "           [0.4040, 0.9183, 0.1746,  ..., 0.2183, 0.5208, 0.7399],\n",
       "           [0.4801, 0.2561, 0.8377,  ..., 0.1270, 0.0251, 0.2027],\n",
       "           [0.1348, 0.5147, 0.6807,  ..., 0.6493, 0.8002, 0.3753]],\n",
       " \n",
       "          [[0.8107, 0.2416, 0.4292,  ..., 0.5550, 0.8582, 0.0404],\n",
       "           [0.1119, 0.0969, 0.1074,  ..., 0.3505, 0.0956, 0.8859],\n",
       "           [0.6951, 0.6472, 0.8232,  ..., 0.5088, 0.4138, 0.6960],\n",
       "           ...,\n",
       "           [0.8312, 0.8732, 0.4684,  ..., 0.7908, 0.5361, 0.6036],\n",
       "           [0.7046, 0.5282, 0.0571,  ..., 0.6097, 0.3041, 0.2682],\n",
       "           [0.2568, 0.8208, 0.8780,  ..., 0.0030, 0.7060, 0.8371]],\n",
       " \n",
       "          [[0.3080, 0.8800, 0.7252,  ..., 0.7502, 0.6287, 0.6455],\n",
       "           [0.6738, 0.0066, 0.2822,  ..., 0.4443, 0.1065, 0.3922],\n",
       "           [0.4397, 0.3000, 0.3173,  ..., 0.3962, 0.8386, 0.0963],\n",
       "           ...,\n",
       "           [0.1223, 0.3749, 0.0625,  ..., 0.7116, 0.5267, 0.1363],\n",
       "           [0.0555, 0.9881, 0.8491,  ..., 0.7665, 0.8583, 0.1825],\n",
       "           [0.6628, 0.3608, 0.5179,  ..., 0.8934, 0.1711, 0.4584]]]]),\n",
       " tensor([[[[ 0.8823,  0.9150,  0.3829,  ...,  0.1587,  0.6542,  0.3278],\n",
       "           [ 0.6532,  0.3958,  0.9147,  ...,  0.2083,  0.3289,  0.1054],\n",
       "           [ 0.9192,  0.4008,  0.9302,  ...,  0.5535,  0.4117,  0.3510],\n",
       "           ...,\n",
       "           [ 0.1525,  0.3970,  0.8703,  ...,  0.1474,  0.6872,  0.9231],\n",
       "           [ 0.5070,  0.9549,  0.0740,  ...,  0.2564,  0.1352,  0.9012],\n",
       "           [ 0.8918,  0.1182,  0.4613,  ...,  0.4078,  0.5411,  0.0410]],\n",
       " \n",
       "          [[ 0.2545,  0.6157,  0.0769,  ...,  0.6907,  0.9170,  0.3514],\n",
       "           [-0.4538,  0.7128,  0.0057,  ...,  0.2422,  0.0621,  0.3856],\n",
       "           [ 0.2987,  0.5236,  0.1308,  ...,  0.2664,  0.2614,  0.0807],\n",
       "           ...,\n",
       "           [-0.1558,  0.1736, -0.4087,  ...,  0.5236,  0.8647,  0.6560],\n",
       "           [-0.0735,  0.4304,  0.0662,  ...,  0.3113,  0.9130,  0.5513],\n",
       "           [-0.3553,  0.3779, -0.1844,  ...,  0.3092,  0.0702,  0.1836]],\n",
       " \n",
       "          [[-0.7107,  0.5309, -0.1554,  ...,  0.8247,  0.9552,  0.7920],\n",
       "           [-0.1052,  0.2167, -0.7293,  ...,  0.5966,  0.0770,  0.8968],\n",
       "           [-0.8098,  0.3451, -0.5593,  ...,  0.9199,  0.2528,  0.9597],\n",
       "           ...,\n",
       "           [-0.4235,  0.3999, -0.6489,  ...,  0.2927,  0.4100,  0.4425],\n",
       "           [-0.5697, -0.0144, -0.6109,  ...,  0.9466,  0.5632,  0.6067],\n",
       "           [-0.9608,  0.0727, -0.6936,  ...,  0.4593,  0.4520,  0.1868]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.2169,  0.4449,  0.2810,  ...,  0.7432,  0.3056,  0.7294],\n",
       "           [ 0.1628,  1.0910,  0.7597,  ...,  0.2685,  0.5729,  0.8347],\n",
       "           [-0.1699,  0.6375,  0.8630,  ...,  0.8091,  0.1490,  0.8784],\n",
       "           ...,\n",
       "           [ 0.0113,  0.2087,  0.3806,  ...,  0.9628,  0.6393,  0.7415],\n",
       "           [-0.2520,  0.5493,  0.9770,  ...,  0.6889,  0.2399,  0.5958],\n",
       "           [ 0.6116,  0.6984,  0.7128,  ...,  0.6756,  0.2053,  0.5029]],\n",
       " \n",
       "          [[-0.9140,  0.0129,  1.1352,  ...,  0.1870,  0.6343,  0.8437],\n",
       "           [-0.5777,  0.8802,  0.6935,  ...,  0.1169,  0.7154,  0.5470],\n",
       "           [-0.1278,  0.1445,  0.9823,  ...,  0.4543,  0.4726,  0.9453],\n",
       "           ...,\n",
       "           [-0.1480,  0.7948,  0.2814,  ...,  0.0173,  0.3140,  0.5673],\n",
       "           [-0.0939,  0.4027,  1.0845,  ...,  0.9522,  0.4345,  0.4085],\n",
       "           [-0.6333,  0.7587,  0.9992,  ...,  0.5803,  0.1328,  0.3634]],\n",
       " \n",
       "          [[-0.5324,  0.1064,  0.5364,  ...,  0.3153,  0.2617,  0.8912],\n",
       "           [-0.9904, -0.2568,  0.1460,  ...,  0.8546,  0.0121,  0.3092],\n",
       "           [-0.9613, -0.0608,  0.5607,  ...,  0.0016,  0.0461,  0.7179],\n",
       "           ...,\n",
       "           [-0.8637,  0.2479,  0.4386,  ...,  0.1975,  0.0344,  0.7407],\n",
       "           [-1.0367, -0.3909,  0.0850,  ...,  0.3175,  0.1066,  0.1090],\n",
       "           [-0.4840, -0.4174,  0.2303,  ...,  0.9116,  0.1591,  0.5712]]],\n",
       " \n",
       " \n",
       "         [[[ 0.3960,  0.0112,  0.0141,  ...,  0.6177,  0.8134,  0.2324],\n",
       "           [ 0.4402,  0.2020,  0.1635,  ...,  0.1304,  0.9454,  0.5923],\n",
       "           [ 0.5934,  0.6982,  0.8041,  ...,  0.6219,  0.6251,  0.3298],\n",
       "           ...,\n",
       "           [ 0.0315,  0.4194,  0.4988,  ...,  0.9535,  0.5774,  0.5616],\n",
       "           [ 0.5357,  0.3242,  0.3692,  ...,  0.7152,  0.0446,  0.9716],\n",
       "           [ 0.5689,  0.3157,  0.9461,  ...,  0.4762,  0.5365,  0.9330]],\n",
       " \n",
       "          [[ 0.0640,  0.8113, -0.5171,  ...,  0.0653,  0.2562,  0.8988],\n",
       "           [-0.5424,  0.8272,  0.3136,  ...,  0.3592,  0.6507,  0.6979],\n",
       "           [ 0.4645,  0.7482,  0.3775,  ...,  0.9802,  0.3773,  0.2867],\n",
       "           ...,\n",
       "           [-0.2853,  1.1141,  0.5581,  ...,  0.7665,  0.1121,  0.3626],\n",
       "           [-0.2761,  0.8298, -0.3818,  ...,  0.2299,  0.7874,  0.1388],\n",
       "           [ 0.1598,  0.9875,  0.2022,  ...,  0.0465,  0.1618,  0.3906]],\n",
       " \n",
       "          [[-0.5832, -0.1522, -0.8476,  ...,  0.6405,  0.1092,  0.4979],\n",
       "           [-0.2927,  0.1192, -0.2426,  ...,  0.7468,  0.8965,  0.4847],\n",
       "           [-0.8554, -0.1961, -0.7744,  ...,  0.7676,  0.9804,  0.9460],\n",
       "           ...,\n",
       "           [-1.0049,  0.4311, -0.3385,  ...,  0.7814,  0.6396,  0.5023],\n",
       "           [-0.8221,  0.6642, -0.3023,  ...,  0.0698,  0.0404,  0.6698],\n",
       "           [-0.6917,  0.7495, -0.2528,  ...,  0.4334,  0.6100,  0.1766]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-0.2045,  0.2653,  0.7822,  ...,  0.2362,  0.9001,  0.6138],\n",
       "           [ 0.1443,  1.0165,  0.4813,  ...,  0.0889,  0.3480,  0.9021],\n",
       "           [ 0.1606,  0.3536,  0.4030,  ...,  0.2694,  0.5836,  0.8626],\n",
       "           ...,\n",
       "           [-0.2987,  0.9578,  0.8541,  ...,  0.2193,  0.5201,  0.7404],\n",
       "           [ 0.1937,  0.5085,  1.1493,  ...,  0.1277,  0.0249,  0.2028],\n",
       "           [-0.2365,  0.4766,  1.0485,  ...,  0.6501,  0.7999,  0.3761]],\n",
       " \n",
       "          [[-0.3569,  0.7669,  0.5649,  ...,  0.5556,  0.8582,  0.0413],\n",
       "           [-0.1121,  0.0966,  0.2441,  ...,  0.3518,  0.0946,  0.8860],\n",
       "           [-0.7414,  0.5936,  0.9791,  ...,  0.5099,  0.4130,  0.6964],\n",
       "           ...,\n",
       "           [-0.9848,  0.6953,  0.6858,  ...,  0.7913,  0.5355,  0.6042],\n",
       "           [-0.6251,  0.6202,  0.1428,  ...,  0.6110,  0.3038,  0.2686],\n",
       "           [-0.8494,  0.1346,  0.9170,  ...,  0.0039,  0.7051,  0.8379]],\n",
       " \n",
       "          [[-0.6433, -0.6748,  0.5873,  ...,  0.7503,  0.6279,  0.6463],\n",
       "           [-0.6166,  0.2717,  0.1366,  ...,  0.4444,  0.1060,  0.3924],\n",
       "           [-0.5243, -0.0921, -0.0626,  ...,  0.3973,  0.8385,  0.0973],\n",
       "           ...,\n",
       "           [-0.2659, -0.2912, -0.2743,  ...,  0.7126,  0.5265,  0.1369],\n",
       "           [-0.4578, -0.8775,  0.7092,  ...,  0.7678,  0.8581,  0.1835],\n",
       "           [-0.7526, -0.0556,  0.1323,  ...,  0.8950,  0.1705,  0.4586]]]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q , q_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len , head_dim = 10, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 8, 32]), torch.Size([2, 10, 8, 64]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_q_complex = torch.view_as_complex(q.float().reshape(*q.shape[:-1], -1, 2))\n",
    "\n",
    "x_q_complex.shape, q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8, 32, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.float().reshape(*q.shape[:-1], -1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.8823, 0.9150],\n",
       "           [0.3829, 0.9593],\n",
       "           [0.3904, 0.6009],\n",
       "           ...,\n",
       "           [0.8913, 0.1447],\n",
       "           [0.5315, 0.1587],\n",
       "           [0.6542, 0.3278]],\n",
       "\n",
       "          [[0.6532, 0.3958],\n",
       "           [0.9147, 0.2036],\n",
       "           [0.2018, 0.2018],\n",
       "           ...,\n",
       "           [0.4654, 0.1612],\n",
       "           [0.1568, 0.2083],\n",
       "           [0.3289, 0.1054]],\n",
       "\n",
       "          [[0.9192, 0.4008],\n",
       "           [0.9302, 0.6558],\n",
       "           [0.0766, 0.8460],\n",
       "           ...,\n",
       "           [0.6870, 0.4121],\n",
       "           [0.3676, 0.5535],\n",
       "           [0.4117, 0.3510]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.1525, 0.3970],\n",
       "           [0.8703, 0.7563],\n",
       "           [0.1836, 0.0991],\n",
       "           ...,\n",
       "           [0.9142, 0.0409],\n",
       "           [0.8343, 0.1474],\n",
       "           [0.6872, 0.9231]],\n",
       "\n",
       "          [[0.5070, 0.9549],\n",
       "           [0.0740, 0.3090],\n",
       "           [0.7916, 0.3911],\n",
       "           ...,\n",
       "           [0.4870, 0.8903],\n",
       "           [0.9807, 0.2564],\n",
       "           [0.1352, 0.9012]],\n",
       "\n",
       "          [[0.8918, 0.1182],\n",
       "           [0.4613, 0.0069],\n",
       "           [0.0907, 0.5966],\n",
       "           ...,\n",
       "           [0.1320, 0.2316],\n",
       "           [0.3901, 0.4078],\n",
       "           [0.5411, 0.0410]]],\n",
       "\n",
       "\n",
       "         [[[0.6556, 0.1186],\n",
       "           [0.1836, 0.0843],\n",
       "           [0.9357, 0.0265],\n",
       "           ...,\n",
       "           [0.2012, 0.0071],\n",
       "           [0.1931, 0.6907],\n",
       "           [0.9170, 0.3513]],\n",
       "\n",
       "          [[0.3546, 0.7670],\n",
       "           [0.2533, 0.2636],\n",
       "           [0.8081, 0.0643],\n",
       "           ...,\n",
       "           [0.3633, 0.2947],\n",
       "           [0.0479, 0.2422],\n",
       "           [0.0622, 0.3856]],\n",
       "\n",
       "          [[0.6020, 0.0316],\n",
       "           [0.9366, 0.8137],\n",
       "           [0.0105, 0.2612],\n",
       "           ...,\n",
       "           [0.2201, 0.9597],\n",
       "           [0.8029, 0.2662],\n",
       "           [0.2614, 0.0806]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.0620, 0.2249],\n",
       "           [0.1381, 0.7480],\n",
       "           [0.1647, 0.4583],\n",
       "           ...,\n",
       "           [0.4956, 0.3696],\n",
       "           [0.4163, 0.5235],\n",
       "           [0.8648, 0.6559]],\n",
       "\n",
       "          [[0.3225, 0.2944],\n",
       "           [0.3762, 0.3067],\n",
       "           [0.9496, 0.7648],\n",
       "           ...,\n",
       "           [0.7867, 0.0252],\n",
       "           [0.1415, 0.3112],\n",
       "           [0.9130, 0.5512]],\n",
       "\n",
       "          [[0.1261, 0.5031],\n",
       "           [0.1117, 0.3905],\n",
       "           [0.3625, 0.9328],\n",
       "           ...,\n",
       "           [0.7842, 0.7776],\n",
       "           [0.0343, 0.3092],\n",
       "           [0.0702, 0.1836]]],\n",
       "\n",
       "\n",
       "         [[[0.7785, 0.4253],\n",
       "           [0.7124, 0.2065],\n",
       "           [0.5760, 0.1976],\n",
       "           ...,\n",
       "           [0.1807, 0.0503],\n",
       "           [0.5326, 0.8245],\n",
       "           [0.9554, 0.7918]],\n",
       "\n",
       "          [[0.2408, 0.0055],\n",
       "           [0.6897, 0.7802],\n",
       "           [0.0707, 0.6793],\n",
       "           ...,\n",
       "           [0.2542, 0.0422],\n",
       "           [0.7651, 0.5963],\n",
       "           [0.0773, 0.8968]],\n",
       "\n",
       "          [[0.6508, 0.5928],\n",
       "           [0.2064, 0.5754],\n",
       "           [0.9818, 0.8429],\n",
       "           ...,\n",
       "           [0.9594, 0.1338],\n",
       "           [0.8214, 0.9196],\n",
       "           [0.2531, 0.9596]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.5398, 0.2187],\n",
       "           [0.2749, 0.6701],\n",
       "           [0.0616, 0.4221],\n",
       "           ...,\n",
       "           [0.2668, 0.5230],\n",
       "           [0.1707, 0.2926],\n",
       "           [0.4102, 0.4424]],\n",
       "\n",
       "          [[0.2240, 0.5240],\n",
       "           [0.4298, 0.6430],\n",
       "           [0.5226, 0.2503],\n",
       "           ...,\n",
       "           [0.9047, 0.4137],\n",
       "           [0.8606, 0.9463],\n",
       "           [0.5633, 0.6065]],\n",
       "\n",
       "          [[0.4659, 0.8434],\n",
       "           [0.0495, 0.6988],\n",
       "           [0.4185, 0.7283],\n",
       "           ...,\n",
       "           [0.8918, 0.8004],\n",
       "           [0.1894, 0.4593],\n",
       "           [0.4520, 0.1866]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0.1288, 0.4779],\n",
       "           [0.1383, 0.2447],\n",
       "           [0.6683, 0.7336],\n",
       "           ...,\n",
       "           [0.0749, 0.3859],\n",
       "           [0.4433, 0.7426],\n",
       "           [0.3062, 0.7291]],\n",
       "\n",
       "          [[0.8395, 0.7156],\n",
       "           [0.7573, 0.4334],\n",
       "           [0.7395, 0.3285],\n",
       "           ...,\n",
       "           [0.7077, 0.1585],\n",
       "           [0.2452, 0.2682],\n",
       "           [0.5737, 0.8341]],\n",
       "\n",
       "          [[0.2907, 0.5922],\n",
       "           [0.8981, 0.4697],\n",
       "           [0.5516, 0.5371],\n",
       "           ...,\n",
       "           [0.1587, 0.8588],\n",
       "           [0.3889, 0.8086],\n",
       "           [0.1498, 0.8783]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.1457, 0.1499],\n",
       "           [0.3298, 0.2467],\n",
       "           [0.5770, 0.8057],\n",
       "           ...,\n",
       "           [0.4767, 0.4062],\n",
       "           [0.2958, 0.9624],\n",
       "           [0.6400, 0.7409]],\n",
       "\n",
       "          [[0.1709, 0.5797],\n",
       "           [0.6340, 0.7596],\n",
       "           [0.3525, 0.4018],\n",
       "           ...,\n",
       "           [0.6468, 0.5556],\n",
       "           [0.2997, 0.6885],\n",
       "           [0.2405, 0.5956]],\n",
       "\n",
       "          [[0.9199, 0.1247],\n",
       "           [0.3573, 0.6168],\n",
       "           [0.5868, 0.5069],\n",
       "           ...,\n",
       "           [0.0405, 0.8917],\n",
       "           [0.3231, 0.6752],\n",
       "           [0.2058, 0.5027]]],\n",
       "\n",
       "\n",
       "         [[[0.1458, 0.9024],\n",
       "           [0.9217, 0.8936],\n",
       "           [0.3250, 0.8112],\n",
       "           ...,\n",
       "           [0.5173, 0.4067],\n",
       "           [0.1574, 0.1868],\n",
       "           [0.6352, 0.8431]],\n",
       "\n",
       "          [[0.9549, 0.4435],\n",
       "           [0.6924, 0.1029],\n",
       "           [0.7664, 0.9778],\n",
       "           ...,\n",
       "           [0.7815, 0.8051],\n",
       "           [0.0222, 0.1168],\n",
       "           [0.7160, 0.5462]],\n",
       "\n",
       "          [[0.1616, 0.1054],\n",
       "           [0.8614, 0.5546],\n",
       "           [0.4989, 0.0166],\n",
       "           ...,\n",
       "           [0.4432, 0.3516],\n",
       "           [0.8222, 0.4531],\n",
       "           [0.4736, 0.9448]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.8079, 0.0307],\n",
       "           [0.1572, 0.4660],\n",
       "           [0.0142, 0.0247],\n",
       "           ...,\n",
       "           [0.4445, 0.3011],\n",
       "           [0.2161, 0.0170],\n",
       "           [0.3146, 0.5670]],\n",
       "\n",
       "          [[0.4120, 0.0343],\n",
       "           [0.8756, 0.8707],\n",
       "           [0.1985, 0.5508],\n",
       "           ...,\n",
       "           [0.1651, 0.7135],\n",
       "           [0.6816, 0.9512],\n",
       "           [0.4349, 0.4081]],\n",
       "\n",
       "          [[0.8428, 0.5161],\n",
       "           [0.9694, 0.2450],\n",
       "           [0.3359, 0.5216],\n",
       "           ...,\n",
       "           [0.4445, 0.9385],\n",
       "           [0.2429, 0.5800],\n",
       "           [0.1331, 0.3633]]],\n",
       "\n",
       "\n",
       "         [[[0.5290, 0.1225],\n",
       "           [0.6358, 0.0706],\n",
       "           [0.2957, 0.8479],\n",
       "           ...,\n",
       "           [0.7670, 0.7137],\n",
       "           [0.3088, 0.3148],\n",
       "           [0.2628, 0.8908]],\n",
       "\n",
       "          [[0.7965, 0.6421],\n",
       "           [0.4776, 0.6249],\n",
       "           [0.9824, 0.6583],\n",
       "           ...,\n",
       "           [0.1972, 0.9945],\n",
       "           [0.4888, 0.8538],\n",
       "           [0.0125, 0.3092]],\n",
       "\n",
       "          [[0.8508, 0.4516],\n",
       "           [0.6684, 0.0811],\n",
       "           [0.9867, 0.2504],\n",
       "           ...,\n",
       "           [0.5146, 0.4133],\n",
       "           [0.3305, 0.0010],\n",
       "           [0.0470, 0.7179]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.8891, 0.1301],\n",
       "           [0.5359, 0.0894],\n",
       "           [0.1553, 0.8839],\n",
       "           ...,\n",
       "           [0.9544, 0.0734],\n",
       "           [0.8865, 0.1961],\n",
       "           [0.0353, 0.7406]],\n",
       "\n",
       "          [[0.7834, 0.7834],\n",
       "           [0.5877, 0.9799],\n",
       "           [0.0693, 0.7307],\n",
       "           ...,\n",
       "           [0.3625, 0.6611],\n",
       "           [0.9405, 0.3160],\n",
       "           [0.1067, 0.1089]],\n",
       "\n",
       "          [[0.2690, 0.5798],\n",
       "           [0.4421, 0.3667],\n",
       "           [0.2663, 0.6301],\n",
       "           ...,\n",
       "           [0.6888, 0.5277],\n",
       "           [0.6767, 0.9105],\n",
       "           [0.1598, 0.5711]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.3960, 0.0112],\n",
       "           [0.0141, 0.4345],\n",
       "           [0.8579, 0.1703],\n",
       "           ...,\n",
       "           [0.7719, 0.0713],\n",
       "           [0.5689, 0.6177],\n",
       "           [0.8134, 0.2324]],\n",
       "\n",
       "          [[0.4402, 0.2020],\n",
       "           [0.1635, 0.7081],\n",
       "           [0.5762, 0.6987],\n",
       "           ...,\n",
       "           [0.3193, 0.4039],\n",
       "           [0.0573, 0.1304],\n",
       "           [0.9454, 0.5923]],\n",
       "\n",
       "          [[0.5934, 0.6982],\n",
       "           [0.8041, 0.8359],\n",
       "           [0.8581, 0.7018],\n",
       "           ...,\n",
       "           [0.4029, 0.8929],\n",
       "           [0.4973, 0.6219],\n",
       "           [0.6251, 0.3298]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.0315, 0.4194],\n",
       "           [0.4988, 0.1735],\n",
       "           [0.7763, 0.5329],\n",
       "           ...,\n",
       "           [0.8044, 0.0438],\n",
       "           [0.3704, 0.9535],\n",
       "           [0.5774, 0.5616]],\n",
       "\n",
       "          [[0.5357, 0.3242],\n",
       "           [0.3692, 0.5332],\n",
       "           [0.8764, 0.3591],\n",
       "           ...,\n",
       "           [0.9743, 0.0718],\n",
       "           [0.1853, 0.7152],\n",
       "           [0.0446, 0.9716]],\n",
       "\n",
       "          [[0.5689, 0.3157],\n",
       "           [0.9461, 0.7331],\n",
       "           [0.9205, 0.8767],\n",
       "           ...,\n",
       "           [0.6511, 0.8030],\n",
       "           [0.6618, 0.4762],\n",
       "           [0.5365, 0.9330]]],\n",
       "\n",
       "\n",
       "         [[[0.7173, 0.3845],\n",
       "           [0.0984, 0.8644],\n",
       "           [0.3138, 0.8117],\n",
       "           ...,\n",
       "           [0.4230, 0.2421],\n",
       "           [0.6126, 0.0652],\n",
       "           [0.2564, 0.8987]],\n",
       "\n",
       "          [[0.4030, 0.9034],\n",
       "           [0.7961, 0.3946],\n",
       "           [0.0976, 0.9015],\n",
       "           ...,\n",
       "           [0.3981, 0.3420],\n",
       "           [0.3294, 0.3591],\n",
       "           [0.6508, 0.6978]],\n",
       "\n",
       "          [[0.8806, 0.0134],\n",
       "           [0.8877, 0.3993],\n",
       "           [0.0114, 0.8269],\n",
       "           ...,\n",
       "           [0.8093, 0.9954],\n",
       "           [0.9867, 0.9800],\n",
       "           [0.3773, 0.2866]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.7833, 0.8420],\n",
       "           [0.9230, 0.1721],\n",
       "           [0.1471, 0.6734],\n",
       "           ...,\n",
       "           [0.2046, 0.3679],\n",
       "           [0.9583, 0.7663],\n",
       "           [0.1121, 0.3626]],\n",
       "\n",
       "          [[0.5491, 0.6807],\n",
       "           [0.2700, 0.8501],\n",
       "           [0.5078, 0.0677],\n",
       "           ...,\n",
       "           [0.9744, 0.9390],\n",
       "           [0.2807, 0.2298],\n",
       "           [0.7875, 0.1387]],\n",
       "\n",
       "          [[0.9174, 0.3991],\n",
       "           [0.4932, 0.2329],\n",
       "           [0.0057, 0.6131],\n",
       "           ...,\n",
       "           [0.0627, 0.3757],\n",
       "           [0.8972, 0.0463],\n",
       "           [0.1619, 0.3906]]],\n",
       "\n",
       "\n",
       "         [[[0.1043, 0.5936],\n",
       "           [0.4006, 0.8782],\n",
       "           [0.0031, 0.5167],\n",
       "           ...,\n",
       "           [0.5864, 0.5811],\n",
       "           [0.1245, 0.6405],\n",
       "           [0.1094, 0.4978]],\n",
       "\n",
       "          [[0.2302, 0.2166],\n",
       "           [0.2073, 0.2580],\n",
       "           [0.2026, 0.2266],\n",
       "           ...,\n",
       "           [0.3290, 0.4130],\n",
       "           [0.2722, 0.7467],\n",
       "           [0.8966, 0.4844]],\n",
       "\n",
       "          [[0.1777, 0.8594],\n",
       "           [0.9069, 0.8409],\n",
       "           [0.4910, 0.6229],\n",
       "           ...,\n",
       "           [0.7205, 0.2477],\n",
       "           [0.3403, 0.7675],\n",
       "           [0.9807, 0.9457]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.8102, 0.7344],\n",
       "           [0.7794, 0.3947],\n",
       "           [0.1044, 0.5058],\n",
       "           ...,\n",
       "           [0.9981, 0.5692],\n",
       "           [0.3335, 0.7813],\n",
       "           [0.6398, 0.5022]],\n",
       "\n",
       "          [[0.9461, 0.4711],\n",
       "           [0.6758, 0.3511],\n",
       "           [0.4093, 0.4785],\n",
       "           ...,\n",
       "           [0.6908, 0.2289],\n",
       "           [0.8248, 0.0695],\n",
       "           [0.0406, 0.6698]],\n",
       "\n",
       "          [[0.9694, 0.3171],\n",
       "           [0.9710, 0.3225],\n",
       "           [0.1200, 0.4223],\n",
       "           ...,\n",
       "           [0.4472, 0.6520],\n",
       "           [0.5685, 0.4332],\n",
       "           [0.6101, 0.1765]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[0.0201, 0.3343],\n",
       "           [0.2341, 0.7709],\n",
       "           [0.5251, 0.1449],\n",
       "           ...,\n",
       "           [0.2159, 0.6424],\n",
       "           [0.9097, 0.2350],\n",
       "           [0.9007, 0.6130]],\n",
       "\n",
       "          [[0.7766, 0.6715],\n",
       "           [0.8157, 0.0746],\n",
       "           [0.7065, 0.0664],\n",
       "           ...,\n",
       "           [0.3058, 0.7735],\n",
       "           [0.1619, 0.0887],\n",
       "           [0.3489, 0.9017]],\n",
       "\n",
       "          [[0.3534, 0.1611],\n",
       "           [0.1453, 0.3825],\n",
       "           [0.8551, 0.6879],\n",
       "           ...,\n",
       "           [0.4245, 0.7426],\n",
       "           [0.3083, 0.2690],\n",
       "           [0.5844, 0.8620]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.4040, 0.9183],\n",
       "           [0.1746, 0.8900],\n",
       "           [0.9361, 0.5817],\n",
       "           ...,\n",
       "           [0.6278, 0.0143],\n",
       "           [0.7826, 0.2183],\n",
       "           [0.5208, 0.7399]],\n",
       "\n",
       "          [[0.4801, 0.2561],\n",
       "           [0.8377, 0.8389],\n",
       "           [0.5487, 0.1875],\n",
       "           ...,\n",
       "           [0.0583, 0.3140],\n",
       "           [0.5614, 0.1270],\n",
       "           [0.0251, 0.2027]],\n",
       "\n",
       "          [[0.1348, 0.5147],\n",
       "           [0.6807, 0.8150],\n",
       "           [0.9996, 0.7613],\n",
       "           ...,\n",
       "           [0.6308, 0.8896],\n",
       "           [0.6078, 0.6493],\n",
       "           [0.8002, 0.3753]]],\n",
       "\n",
       "\n",
       "         [[[0.8107, 0.2416],\n",
       "           [0.4292, 0.5459],\n",
       "           [0.8456, 0.7204],\n",
       "           ...,\n",
       "           [0.5523, 0.3194],\n",
       "           [0.4672, 0.5550],\n",
       "           [0.8582, 0.0404]],\n",
       "\n",
       "          [[0.1119, 0.0969],\n",
       "           [0.1074, 0.5032],\n",
       "           [0.7777, 0.6307],\n",
       "           ...,\n",
       "           [0.4382, 0.0811],\n",
       "           [0.9236, 0.3505],\n",
       "           [0.0956, 0.8859]],\n",
       "\n",
       "          [[0.6951, 0.6472],\n",
       "           [0.8232, 0.6742],\n",
       "           [0.3912, 0.4131],\n",
       "           ...,\n",
       "           [0.7812, 0.5243],\n",
       "           [0.7654, 0.5088],\n",
       "           [0.4138, 0.6960]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.8312, 0.8732],\n",
       "           [0.4684, 0.8429],\n",
       "           [0.7875, 0.4256],\n",
       "           ...,\n",
       "           [0.4893, 0.4467],\n",
       "           [0.3595, 0.7908],\n",
       "           [0.5361, 0.6036]],\n",
       "\n",
       "          [[0.7046, 0.5282],\n",
       "           [0.0571, 0.3141],\n",
       "           [0.8140, 0.7868],\n",
       "           ...,\n",
       "           [0.8564, 0.4342],\n",
       "           [0.8888, 0.6097],\n",
       "           [0.3041, 0.2682]],\n",
       "\n",
       "          [[0.2568, 0.8208],\n",
       "           [0.8780, 0.2649],\n",
       "           [0.0436, 0.7834],\n",
       "           ...,\n",
       "           [0.9167, 0.5627],\n",
       "           [0.6735, 0.0030],\n",
       "           [0.7060, 0.8371]]],\n",
       "\n",
       "\n",
       "         [[[0.3080, 0.8800],\n",
       "           [0.7252, 0.1349],\n",
       "           [0.1252, 0.5070],\n",
       "           ...,\n",
       "           [0.6458, 0.2902],\n",
       "           [0.0227, 0.7502],\n",
       "           [0.6287, 0.6455]],\n",
       "\n",
       "          [[0.6738, 0.0066],\n",
       "           [0.2822, 0.2571],\n",
       "           [0.9784, 0.2893],\n",
       "           ...,\n",
       "           [0.8080, 0.7949],\n",
       "           [0.0493, 0.4443],\n",
       "           [0.1065, 0.3922]],\n",
       "\n",
       "          [[0.4397, 0.3000],\n",
       "           [0.3173, 0.7704],\n",
       "           [0.4852, 0.0025],\n",
       "           ...,\n",
       "           [0.4945, 0.9553],\n",
       "           [0.7235, 0.3962],\n",
       "           [0.8386, 0.0963]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.1223, 0.3749],\n",
       "           [0.0625, 0.7350],\n",
       "           [0.2795, 0.2491],\n",
       "           ...,\n",
       "           [0.7514, 0.7017],\n",
       "           [0.6790, 0.7116],\n",
       "           [0.5267, 0.1363]],\n",
       "\n",
       "          [[0.0555, 0.9881],\n",
       "           [0.8491, 0.1099],\n",
       "           [0.8918, 0.0324],\n",
       "           ...,\n",
       "           [0.2573, 0.9620],\n",
       "           [0.7993, 0.7665],\n",
       "           [0.8583, 0.1825]],\n",
       "\n",
       "          [[0.6628, 0.3608],\n",
       "           [0.5179, 0.7355],\n",
       "           [0.3340, 0.9906],\n",
       "           ...,\n",
       "           [0.3964, 0.5748],\n",
       "           [0.9908, 0.8934],\n",
       "           [0.1711, 0.4584]]]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.float().reshape(*q.shape[:-1], -1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8823+0.9150j, 0.3829+0.9593j, 0.3904+0.6009j,  ...,\n",
       "           0.8913+0.1447j, 0.5315+0.1587j, 0.6542+0.3278j],\n",
       "          [0.6532+0.3958j, 0.9147+0.2036j, 0.2018+0.2018j,  ...,\n",
       "           0.4654+0.1612j, 0.1568+0.2083j, 0.3289+0.1054j],\n",
       "          [0.9192+0.4008j, 0.9302+0.6558j, 0.0766+0.8460j,  ...,\n",
       "           0.6870+0.4121j, 0.3676+0.5535j, 0.4117+0.3510j],\n",
       "          ...,\n",
       "          [0.1525+0.3970j, 0.8703+0.7563j, 0.1836+0.0991j,  ...,\n",
       "           0.9142+0.0409j, 0.8343+0.1474j, 0.6872+0.9231j],\n",
       "          [0.5070+0.9549j, 0.0740+0.3090j, 0.7916+0.3911j,  ...,\n",
       "           0.4870+0.8903j, 0.9807+0.2564j, 0.1352+0.9012j],\n",
       "          [0.8918+0.1182j, 0.4613+0.0069j, 0.0907+0.5966j,  ...,\n",
       "           0.1320+0.2316j, 0.3901+0.4078j, 0.5411+0.0410j]],\n",
       "\n",
       "         [[0.6556+0.1186j, 0.1836+0.0843j, 0.9357+0.0265j,  ...,\n",
       "           0.2012+0.0071j, 0.1931+0.6907j, 0.9170+0.3513j],\n",
       "          [0.3546+0.7670j, 0.2533+0.2636j, 0.8081+0.0643j,  ...,\n",
       "           0.3633+0.2947j, 0.0479+0.2422j, 0.0622+0.3856j],\n",
       "          [0.6020+0.0316j, 0.9366+0.8137j, 0.0105+0.2612j,  ...,\n",
       "           0.2201+0.9597j, 0.8029+0.2662j, 0.2614+0.0806j],\n",
       "          ...,\n",
       "          [0.0620+0.2249j, 0.1381+0.7480j, 0.1647+0.4583j,  ...,\n",
       "           0.4956+0.3696j, 0.4163+0.5235j, 0.8648+0.6559j],\n",
       "          [0.3225+0.2944j, 0.3762+0.3067j, 0.9496+0.7648j,  ...,\n",
       "           0.7867+0.0252j, 0.1415+0.3112j, 0.9130+0.5512j],\n",
       "          [0.1261+0.5031j, 0.1117+0.3905j, 0.3625+0.9328j,  ...,\n",
       "           0.7842+0.7776j, 0.0343+0.3092j, 0.0702+0.1836j]],\n",
       "\n",
       "         [[0.7785+0.4253j, 0.7124+0.2065j, 0.5760+0.1976j,  ...,\n",
       "           0.1807+0.0503j, 0.5326+0.8245j, 0.9554+0.7918j],\n",
       "          [0.2408+0.0055j, 0.6897+0.7802j, 0.0707+0.6793j,  ...,\n",
       "           0.2542+0.0422j, 0.7651+0.5963j, 0.0773+0.8968j],\n",
       "          [0.6508+0.5928j, 0.2064+0.5754j, 0.9818+0.8429j,  ...,\n",
       "           0.9594+0.1338j, 0.8214+0.9196j, 0.2531+0.9596j],\n",
       "          ...,\n",
       "          [0.5398+0.2187j, 0.2749+0.6701j, 0.0616+0.4221j,  ...,\n",
       "           0.2668+0.5230j, 0.1707+0.2926j, 0.4102+0.4424j],\n",
       "          [0.2240+0.5240j, 0.4298+0.6430j, 0.5226+0.2503j,  ...,\n",
       "           0.9047+0.4137j, 0.8606+0.9463j, 0.5633+0.6065j],\n",
       "          [0.4659+0.8434j, 0.0495+0.6988j, 0.4185+0.7283j,  ...,\n",
       "           0.8918+0.8004j, 0.1894+0.4593j, 0.4520+0.1866j]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.1288+0.4779j, 0.1383+0.2447j, 0.6683+0.7336j,  ...,\n",
       "           0.0749+0.3859j, 0.4433+0.7426j, 0.3062+0.7291j],\n",
       "          [0.8395+0.7156j, 0.7573+0.4334j, 0.7395+0.3285j,  ...,\n",
       "           0.7077+0.1585j, 0.2452+0.2682j, 0.5737+0.8341j],\n",
       "          [0.2907+0.5922j, 0.8981+0.4697j, 0.5516+0.5371j,  ...,\n",
       "           0.1587+0.8588j, 0.3889+0.8086j, 0.1498+0.8783j],\n",
       "          ...,\n",
       "          [0.1457+0.1499j, 0.3298+0.2467j, 0.5770+0.8057j,  ...,\n",
       "           0.4767+0.4062j, 0.2958+0.9624j, 0.6400+0.7409j],\n",
       "          [0.1709+0.5797j, 0.6340+0.7596j, 0.3525+0.4018j,  ...,\n",
       "           0.6468+0.5556j, 0.2997+0.6885j, 0.2405+0.5956j],\n",
       "          [0.9199+0.1247j, 0.3573+0.6168j, 0.5868+0.5069j,  ...,\n",
       "           0.0405+0.8917j, 0.3231+0.6752j, 0.2058+0.5027j]],\n",
       "\n",
       "         [[0.1458+0.9024j, 0.9217+0.8936j, 0.3250+0.8112j,  ...,\n",
       "           0.5173+0.4067j, 0.1574+0.1868j, 0.6352+0.8431j],\n",
       "          [0.9549+0.4435j, 0.6924+0.1029j, 0.7664+0.9778j,  ...,\n",
       "           0.7815+0.8051j, 0.0222+0.1168j, 0.7160+0.5462j],\n",
       "          [0.1616+0.1054j, 0.8614+0.5546j, 0.4989+0.0166j,  ...,\n",
       "           0.4432+0.3516j, 0.8222+0.4531j, 0.4736+0.9448j],\n",
       "          ...,\n",
       "          [0.8079+0.0307j, 0.1572+0.4660j, 0.0142+0.0247j,  ...,\n",
       "           0.4445+0.3011j, 0.2161+0.0170j, 0.3146+0.5670j],\n",
       "          [0.4120+0.0343j, 0.8756+0.8707j, 0.1985+0.5508j,  ...,\n",
       "           0.1651+0.7135j, 0.6816+0.9512j, 0.4349+0.4081j],\n",
       "          [0.8428+0.5161j, 0.9694+0.2450j, 0.3359+0.5216j,  ...,\n",
       "           0.4445+0.9385j, 0.2429+0.5800j, 0.1331+0.3633j]],\n",
       "\n",
       "         [[0.5290+0.1225j, 0.6358+0.0706j, 0.2957+0.8479j,  ...,\n",
       "           0.7670+0.7137j, 0.3088+0.3148j, 0.2628+0.8908j],\n",
       "          [0.7965+0.6421j, 0.4776+0.6249j, 0.9824+0.6583j,  ...,\n",
       "           0.1972+0.9945j, 0.4888+0.8538j, 0.0125+0.3092j],\n",
       "          [0.8508+0.4516j, 0.6684+0.0811j, 0.9867+0.2504j,  ...,\n",
       "           0.5146+0.4133j, 0.3305+0.0010j, 0.0470+0.7179j],\n",
       "          ...,\n",
       "          [0.8891+0.1301j, 0.5359+0.0894j, 0.1553+0.8839j,  ...,\n",
       "           0.9544+0.0734j, 0.8865+0.1961j, 0.0353+0.7406j],\n",
       "          [0.7834+0.7834j, 0.5877+0.9799j, 0.0693+0.7307j,  ...,\n",
       "           0.3625+0.6611j, 0.9405+0.3160j, 0.1067+0.1089j],\n",
       "          [0.2690+0.5798j, 0.4421+0.3667j, 0.2663+0.6301j,  ...,\n",
       "           0.6888+0.5277j, 0.6767+0.9105j, 0.1598+0.5711j]]],\n",
       "\n",
       "\n",
       "        [[[0.3960+0.0112j, 0.0141+0.4345j, 0.8579+0.1703j,  ...,\n",
       "           0.7719+0.0713j, 0.5689+0.6177j, 0.8134+0.2324j],\n",
       "          [0.4402+0.2020j, 0.1635+0.7081j, 0.5762+0.6987j,  ...,\n",
       "           0.3193+0.4039j, 0.0573+0.1304j, 0.9454+0.5923j],\n",
       "          [0.5934+0.6982j, 0.8041+0.8359j, 0.8581+0.7018j,  ...,\n",
       "           0.4029+0.8929j, 0.4973+0.6219j, 0.6251+0.3298j],\n",
       "          ...,\n",
       "          [0.0315+0.4194j, 0.4988+0.1735j, 0.7763+0.5329j,  ...,\n",
       "           0.8044+0.0438j, 0.3704+0.9535j, 0.5774+0.5616j],\n",
       "          [0.5357+0.3242j, 0.3692+0.5332j, 0.8764+0.3591j,  ...,\n",
       "           0.9743+0.0718j, 0.1853+0.7152j, 0.0446+0.9716j],\n",
       "          [0.5689+0.3157j, 0.9461+0.7331j, 0.9205+0.8767j,  ...,\n",
       "           0.6511+0.8030j, 0.6618+0.4762j, 0.5365+0.9330j]],\n",
       "\n",
       "         [[0.7173+0.3845j, 0.0984+0.8644j, 0.3138+0.8117j,  ...,\n",
       "           0.4230+0.2421j, 0.6126+0.0652j, 0.2564+0.8987j],\n",
       "          [0.4030+0.9034j, 0.7961+0.3946j, 0.0976+0.9015j,  ...,\n",
       "           0.3981+0.3420j, 0.3294+0.3591j, 0.6508+0.6978j],\n",
       "          [0.8806+0.0134j, 0.8877+0.3993j, 0.0114+0.8269j,  ...,\n",
       "           0.8093+0.9954j, 0.9867+0.9800j, 0.3773+0.2866j],\n",
       "          ...,\n",
       "          [0.7833+0.8420j, 0.9230+0.1721j, 0.1471+0.6734j,  ...,\n",
       "           0.2046+0.3679j, 0.9583+0.7663j, 0.1121+0.3626j],\n",
       "          [0.5491+0.6807j, 0.2700+0.8501j, 0.5078+0.0677j,  ...,\n",
       "           0.9744+0.9390j, 0.2807+0.2298j, 0.7875+0.1387j],\n",
       "          [0.9174+0.3991j, 0.4932+0.2329j, 0.0057+0.6131j,  ...,\n",
       "           0.0627+0.3757j, 0.8972+0.0463j, 0.1619+0.3906j]],\n",
       "\n",
       "         [[0.1043+0.5936j, 0.4006+0.8782j, 0.0031+0.5167j,  ...,\n",
       "           0.5864+0.5811j, 0.1245+0.6405j, 0.1094+0.4978j],\n",
       "          [0.2302+0.2166j, 0.2073+0.2580j, 0.2026+0.2266j,  ...,\n",
       "           0.3290+0.4130j, 0.2722+0.7467j, 0.8966+0.4844j],\n",
       "          [0.1777+0.8594j, 0.9069+0.8409j, 0.4910+0.6229j,  ...,\n",
       "           0.7205+0.2477j, 0.3403+0.7675j, 0.9807+0.9457j],\n",
       "          ...,\n",
       "          [0.8102+0.7344j, 0.7794+0.3947j, 0.1044+0.5058j,  ...,\n",
       "           0.9981+0.5692j, 0.3335+0.7813j, 0.6398+0.5022j],\n",
       "          [0.9461+0.4711j, 0.6758+0.3511j, 0.4093+0.4785j,  ...,\n",
       "           0.6908+0.2289j, 0.8248+0.0695j, 0.0406+0.6698j],\n",
       "          [0.9694+0.3171j, 0.9710+0.3225j, 0.1200+0.4223j,  ...,\n",
       "           0.4472+0.6520j, 0.5685+0.4332j, 0.6101+0.1765j]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0201+0.3343j, 0.2341+0.7709j, 0.5251+0.1449j,  ...,\n",
       "           0.2159+0.6424j, 0.9097+0.2350j, 0.9007+0.6130j],\n",
       "          [0.7766+0.6715j, 0.8157+0.0746j, 0.7065+0.0664j,  ...,\n",
       "           0.3058+0.7735j, 0.1619+0.0887j, 0.3489+0.9017j],\n",
       "          [0.3534+0.1611j, 0.1453+0.3825j, 0.8551+0.6879j,  ...,\n",
       "           0.4245+0.7426j, 0.3083+0.2690j, 0.5844+0.8620j],\n",
       "          ...,\n",
       "          [0.4040+0.9183j, 0.1746+0.8900j, 0.9361+0.5817j,  ...,\n",
       "           0.6278+0.0143j, 0.7826+0.2183j, 0.5208+0.7399j],\n",
       "          [0.4801+0.2561j, 0.8377+0.8389j, 0.5487+0.1875j,  ...,\n",
       "           0.0583+0.3140j, 0.5614+0.1270j, 0.0251+0.2027j],\n",
       "          [0.1348+0.5147j, 0.6807+0.8150j, 0.9996+0.7613j,  ...,\n",
       "           0.6308+0.8896j, 0.6078+0.6493j, 0.8002+0.3753j]],\n",
       "\n",
       "         [[0.8107+0.2416j, 0.4292+0.5459j, 0.8456+0.7204j,  ...,\n",
       "           0.5523+0.3194j, 0.4672+0.5550j, 0.8582+0.0404j],\n",
       "          [0.1119+0.0969j, 0.1074+0.5032j, 0.7777+0.6307j,  ...,\n",
       "           0.4382+0.0811j, 0.9236+0.3505j, 0.0956+0.8859j],\n",
       "          [0.6951+0.6472j, 0.8232+0.6742j, 0.3912+0.4131j,  ...,\n",
       "           0.7812+0.5243j, 0.7654+0.5088j, 0.4138+0.6960j],\n",
       "          ...,\n",
       "          [0.8312+0.8732j, 0.4684+0.8429j, 0.7875+0.4256j,  ...,\n",
       "           0.4893+0.4467j, 0.3595+0.7908j, 0.5361+0.6036j],\n",
       "          [0.7046+0.5282j, 0.0571+0.3141j, 0.8140+0.7868j,  ...,\n",
       "           0.8564+0.4342j, 0.8888+0.6097j, 0.3041+0.2682j],\n",
       "          [0.2568+0.8208j, 0.8780+0.2649j, 0.0436+0.7834j,  ...,\n",
       "           0.9167+0.5627j, 0.6735+0.0030j, 0.7060+0.8371j]],\n",
       "\n",
       "         [[0.3080+0.8800j, 0.7252+0.1349j, 0.1252+0.5070j,  ...,\n",
       "           0.6458+0.2902j, 0.0227+0.7502j, 0.6287+0.6455j],\n",
       "          [0.6738+0.0066j, 0.2822+0.2571j, 0.9784+0.2893j,  ...,\n",
       "           0.8080+0.7949j, 0.0493+0.4443j, 0.1065+0.3922j],\n",
       "          [0.4397+0.3000j, 0.3173+0.7704j, 0.4852+0.0025j,  ...,\n",
       "           0.4945+0.9553j, 0.7235+0.3962j, 0.8386+0.0963j],\n",
       "          ...,\n",
       "          [0.1223+0.3749j, 0.0625+0.7350j, 0.2795+0.2491j,  ...,\n",
       "           0.7514+0.7017j, 0.6790+0.7116j, 0.5267+0.1363j],\n",
       "          [0.0555+0.9881j, 0.8491+0.1099j, 0.8918+0.0324j,  ...,\n",
       "           0.2573+0.9620j, 0.7993+0.7665j, 0.8583+0.1825j],\n",
       "          [0.6628+0.3608j, 0.5179+0.7355j, 0.3340+0.9906j,  ...,\n",
       "           0.3964+0.5748j, 0.9908+0.8934j, 0.1711+0.4584j]]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.view_as_complex(q.float().reshape(*q.shape[:-1], -1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis[:seq_len].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1, 32])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis.unsqueeze(0).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 32]), torch.Size([1, 10, 1, 32]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs_cis.unsqueeze(0).shape , freqs_cis.unsqueeze(0).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8, 32])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_q_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 8, 32]), torch.Size([1, 10, 1, 32]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_q_complex.shape , freqs_cis.unsqueeze(0).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 256])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_q_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).flatten(-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8, 32, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.view_as_real(x_q_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 8, 32])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_q_complex * freqs_cis.unsqueeze(0).unsqueeze(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
